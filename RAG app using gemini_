{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPIUIUCR/S7NBpkzElK2FFE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":[],"metadata":{"id":"MJJoiUrgACsI"}},{"cell_type":"markdown","source":["# New Section"],"metadata":{"id":"P1Ue9tYCAiNg"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2bAGT3LMwUmr","executionInfo":{"status":"ok","timestamp":1738853894102,"user_tz":-330,"elapsed":3493,"user":{"displayName":"Brue Pl","userId":"14478803105777518815"}},"outputId":"cae9db94-7bef-4a3c-ef65-336958bb430e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.16)\n","Collecting langchain-google-genai\n","  Downloading langchain_google_genai-2.0.9-py3-none-any.whl.metadata (3.6 kB)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.37)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.11)\n","Requirement already satisfied: langchain-core<0.4.0,>=0.3.32 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.32)\n","Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.5)\n","Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.2)\n","Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.4)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n","Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n","  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n","Requirement already satisfied: google-generativeai<0.9.0,>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.8.4)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n","Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.6.15)\n","Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.19.2)\n","Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.155.0)\n","Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.27.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.25.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.67.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.12.2)\n","Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.26.0)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.32->langchain) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.32->langchain) (24.2)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2024.12.14)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n","Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.66.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (5.5.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.4.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.9)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.7.1)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.32->langchain) (3.0.0)\n","Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.22.0)\n","Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.2.0)\n","Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.1.1)\n","Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.70.0)\n","Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.62.3)\n","Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (3.2.1)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.6.1)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n","Downloading langchain_google_genai-2.0.9-py3-none-any.whl (41 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n","Installing collected packages: filetype, langchain-google-genai\n","Successfully installed filetype-1.2.0 langchain-google-genai-2.0.9\n"]}],"source":["!pip install langchain langchain-google-genai"]},{"cell_type":"code","source":["from langchain_google_genai import ChatGoogleGenerativeAI"],"metadata":{"id":"04lWOJmAxIR1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import userdata"],"metadata":{"id":"UHpOlDNP0QQ9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["llm  = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", google_api_key= userdata.get('GOOGLE_API_KEY'))\n","print(llm)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WxrPWuZDxQwD","executionInfo":{"status":"ok","timestamp":1738854938752,"user_tz":-330,"elapsed":3177,"user":{"displayName":"Brue Pl","userId":"14478803105777518815"}},"outputId":"21cbf4b4-6bfb-4b02-c90b-5254f4a7f6c8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["model='models/gemini-2.0-flash' google_api_key=SecretStr('**********') client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7b6ab2fdc490> default_metadata=()\n"]}]},{"cell_type":"code","source":["result = llm.invoke(\"what is the capital of india?\")\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pO9vPn68zDMo","executionInfo":{"status":"ok","timestamp":1738854942407,"user_tz":-330,"elapsed":2124,"user":{"displayName":"Brue Pl","userId":"14478803105777518815"}},"outputId":"420d62e5-4c07-4f85-fd87-2e41daa6efef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["content='The capital of India is **New Delhi**.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-ba247488-1568-4766-92d1-fe03cf411a4d-0' usage_metadata={'input_tokens': 7, 'output_tokens': 10, 'total_tokens': 17, 'input_token_details': {'cache_read': 0}}\n"]}]},{"cell_type":"code","source":["!pip install langchain-community\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RIC3R7cn0eG5","executionInfo":{"status":"ok","timestamp":1738854978276,"user_tz":-330,"elapsed":6225,"user":{"displayName":"Brue Pl","userId":"14478803105777518815"}},"outputId":"e5f1843f-842d-4b35-efa0-81aee03850d7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchain-community\n","  Downloading langchain_community-0.3.16-py3-none-any.whl.metadata (2.9 kB)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.37)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.11)\n","Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n","  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n","Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n","  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n","Requirement already satisfied: langchain<0.4.0,>=0.3.16 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.16)\n","Requirement already satisfied: langchain-core<0.4.0,>=0.3.32 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.32)\n","Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.2)\n","Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\n","Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n","  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n","  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n","Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.16->langchain-community) (0.3.5)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.16->langchain-community) (2.10.6)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.32->langchain-community) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.32->langchain-community) (24.2)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.32->langchain-community) (4.12.2)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.15)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n","Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2024.12.14)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (3.7.1)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.32->langchain-community) (3.0.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.16->langchain-community) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.16->langchain-community) (2.27.2)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n","Downloading langchain_community-0.3.16-py3-none-any.whl (2.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n","Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n","Downloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n","Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n","Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.16 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.7.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n"]}]},{"cell_type":"code","source":["!pip install faiss-cpu"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HwWQM_421Tct","executionInfo":{"status":"ok","timestamp":1738854996931,"user_tz":-330,"elapsed":4644,"user":{"displayName":"Brue Pl","userId":"14478803105777518815"}},"outputId":"4db3c481-d1f8-4bc4-a989-3c68240759eb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting faiss-cpu\n","  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n","Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n","Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: faiss-cpu\n","Successfully installed faiss-cpu-1.10.0\n"]}]},{"cell_type":"code","source":["!mkdir data\n"],"metadata":{"id":"fRq7Fecq1YRV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install pyPDF"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_PEJQxlJ2Oaq","executionInfo":{"status":"ok","timestamp":1738855233913,"user_tz":-330,"elapsed":2822,"user":{"displayName":"Brue Pl","userId":"14478803105777518815"}},"outputId":"4d6e59c9-3337-4b08-8ba9-9485515ff380"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyPDF\n","  Downloading pypdf-5.2.0-py3-none-any.whl.metadata (7.2 kB)\n","Downloading pypdf-5.2.0-py3-none-any.whl (298 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/298.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.7/298.7 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pyPDF\n","Successfully installed pyPDF-5.2.0\n"]}]},{"cell_type":"code","source":["from langchain.document_loaders import PyPDFLoader\n","documents = PyPDFLoader('/content/data/2307.08691v1.pdf').load()"],"metadata":{"id":"MLd_4T5J1hd8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["documents"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xwVU6y9V2cyj","executionInfo":{"status":"ok","timestamp":1738855278486,"user_tz":-330,"elapsed":747,"user":{"displayName":"Brue Pl","userId":"14478803105777518815"}},"outputId":"739110c2-4218-4d9c-8ddc-23eac27d8467"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 0, 'page_label': '1'}, page_content='FlashAttention-2:\\nFaster Attention with Better Parallelism and Work Partitioning\\nTri Dao1,2\\n1Department of Computer Science, Princeton University\\n2Department of Computer Science, Stanford University\\ntrid@cs.stanford.edu\\nJuly 18, 2023\\nAbstract\\nScaling Transformers to longer sequence lengths has been a major problem in the last several years,\\npromising to improve performance in language modeling and high-resolution image understanding, as\\nwell as to unlock new applications in code, audio, and video generation. The attention layer is the\\nmain bottleneck in scaling to longer sequences, as its runtime and memory increase quadratically in\\nthe sequence length. FlashAttention [5] exploits the asymmetric GPU memory hierarchy to bring\\nsignificant memory saving (linear instead of quadratic) and runtime speedup (2-4×compared to optimized\\nbaselines), with no approximation. However,FlashAttention is still not nearly as fast as optimized\\nmatrix-multiply (GEMM) operations, reaching only 25-40% of the theoretical maximum FLOPs/s. We\\nobserve that the inefficiency is due to suboptimal work partitioning between different thread blocks and\\nwarps on the GPU, causing either low-occupancy or unnecessary shared memory reads/writes. We propose\\nFlashAttention-2, with better work partitioning to address these issues. In particular, we (1) tweak\\nthe algorithm to reduce the number of non-matmul FLOPs (2) parallelize the attention computation, even\\nfor a single head, across different thread blocks to increase occupancy, and (3) within each thread block,\\ndistribute the work between warps to reduce communication through shared memory. These yield around\\n2×speedup compared toFlashAttention, reaching 50-73% of the theoretical maximum FLOPs/s on\\nA100 and getting close to the efficiency of GEMM operations. We empirically validate that when used\\nend-to-end to train GPT-style models,FlashAttention-2 reaches training speed of up to 225 TFLOPs/s\\nper A100 GPU (72% model FLOPs utilization).1\\n1 Introduction\\nScaling up the context length of Transformers [18] is a challenge, since the attention layer at their heart\\nhas runtime and memory requirements quadratic in the input sequence length. Ideally, we would like to go\\nbeyond the standard 2k sequence length limit to train models to understand books, high resolution images,\\nand long-form videos. Just within the last year, there have been several language models with much longer\\ncontext than before: GPT-4 [12] with context length 32k, MosaicML’s MPT with context length 65k, and\\nAnthropic’s Claude with context length 100k. Emerging use cases such as long document querying and story\\nwriting have demonstrated a need for models with such long context.\\nTo reduce the computational requirement of attention on such long context, there have been numerous\\nmethods proposed to approximate attention [2, 3, 4, 8, 9, 14, 19, 20]. Though these methods have seen\\nsome use cases, as far as we know, most large-scale training runs still use standard attention. Motivated by\\nthis, Dao et al.[5] proposed to reorder the attention computation and leverages classical techniques (tiling,\\nrecomputation) to significantly speed it up and reduce memory usage from quadratic to linear in sequence\\nlength. This yields 2-4×wall-clock time speedup over optimized baselines, up to 10-20×memory saving,\\n1FlashAttention-2 is available athttps://github.com/Dao-AILab/flash-attention\\n1\\narXiv:2307.08691v1  [cs.LG]  17 Jul 2023'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 1, 'page_label': '2'}, page_content='with no approximation, and as a resultFlashAttention has seen wide adoption in large-scale training and\\ninference of Transformers.\\nHowever, context length increases even more,FlashAttention is still not nearly as efficient as other\\nprimitives such as matrix-multiply (GEMM). In particular, whileFlashAttention is already 2-4×faster\\nthan a standard attention implementation, the forward pass only reaches 30-50% of the theoretical maximum\\nFLOPs/s of the device (Fig. 5), while the backward pass is even more challenging, reaching only 25-35%\\nof maximum throughput on A100 GPU (Fig. 6). In contrast, optimized GEMM can reach up to 80-90% of\\nthe theoretical maximum device throughput. Through careful profiling, we observe thatFlashAttention\\nstill has suboptimal work partitioning between different thread blocks and warps on the GPU, causing either\\nlow-occupancy or unnecessary shared memory reads/writes.\\nBuilding on FlashAttention, we propose FlashAttention-2 with better parallelism and work\\npartitioning to address these challenges.\\n1. In Section 3.1, we tweak the algorithms to reduce the number of non-matmul FLOPs while not changing\\nthe output. While the non-matmul FLOPs only account for a small fraction of the total FLOPs, they\\ntake longer to perform as GPUs have specialized units for matrix multiply, and as a result the matmul\\nthroughput can be up to 16×higher than non-matmul throughput. It is thus important to reduce\\nnon-matmul FLOPs and spend as much time as possible doing matmul FLOPs.\\n2. We propose to parallelize both the forward pass and backward pass along the sequence length dimension,\\nin addition to the batch and number of heads dimension. This increases occupancy (utilization of GPU\\nresources) in the case where the sequences are long (and hence batch size is often small).\\n3. Even within one block of attention computation, we partition the work between different warps of a\\nthread block to reduce communication and shared memory reads/writes.\\nIn Section 4, we empirically validate thatFlashAttention-2 yields significant speedup compared to\\neven FlashAttention. Benchmarks on different settings (with or without causal mask, different head\\ndimensions) show thatFlashAttention-2 achieves around 2×speedup overFlashAttention, reaching\\nup to 73% of the theoretical max throughput in the forward pass, and up to 63% of the theoretical max\\nthroughput in the backward pass. When used end-to-end to train GPT-style models, we reach training speed\\nof up to 225 TFLOPs/s per A100 GPU.\\n2 Background\\nWe provide some background on the performance characteristics and execution model of GPUs. We also\\ndescribe the standard implementation of attention, as well asFlashAttention.\\n2.1 Hardware characteristics\\nGPU performance characteristics.The GPU consists of compute elements (e.g., floating point arithmetic\\nunits) and a memory hierarchy. Most modern GPUs contain specialized units to accelerate matrix multiply in\\nlow-precision (e.g., Tensor Cores on Nvidia GPUs for FP16/BF16 matrix multiply). The memory hierarchy\\ncomprise of high bandwidth memory (HBM), and on-chip SRAM (aka shared memory). As an example, the\\nA100 GPU has 40-80GB of high bandwidth memory (HBM) with bandwidth 1.5-2.0TB/s and 192KB of\\non-chip SRAM per each of 108 streaming multiprocessors with bandwidth estimated around 19TB/s [6, 7].\\nAs the L2 cache is not directly controllable by the programmer, we focus on the HBM and SRAM for the\\npurpose of this discussion.\\nExecution Model. GPUs have a massive number of threads to execute an operation (called a kernel).\\nThreads are organized into thread blocks, which are scheduled to run on streaming multiprocessors (SMs).\\nWithin each thread blocks, threads are grouped into warps (a group of 32 threads). Threads within a warp\\ncan communicate by fast shuffle instructions or cooperate to perform matrix multiply. Warps within a thread\\nblock can communicate by reading from / writing to shared memory. Each kernel loads inputs from HBM to\\nregisters and SRAM, computes, then writes outputs to HBM.\\n2'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 2, 'page_label': '3'}, page_content='2.2 Standard Attention Implementation\\nGiven input sequencesQ,K,V ∈R𝑁×𝑑 where 𝑁 is the sequence length and𝑑 is the head dimension, we want\\nto compute the attention outputO ∈R𝑁×𝑑:\\nS = QK⊤∈R𝑁×𝑁, P = softmax(S)∈ R𝑁×𝑁, O = PV ∈R𝑁×𝑑,\\nwhere softmax is applied row-wise.2 For multi-head attention (MHA), this same computation is performed in\\nparallel across many heads, and parallel over the batch dimension (number of input sequences in a batch).\\nThe backward pass of attention proceeds as follows. LetdO ∈R𝑁×𝑑 be the gradient ofO with respect to\\nsome loss function. Then by the chain rule (aka backpropagation):\\ndV = P⊤dO ∈R𝑁×𝑑\\ndP = dOV⊤∈R𝑁×𝑁\\ndS = dsoftmax(dP)∈ R𝑁×𝑁\\ndQ = dSK ∈R𝑁×𝑑\\ndK = QdS⊤∈R𝑁×𝑑,\\nwhere dsoftmax is the gradient (backward pass) of softmax applied row-wise. One can work out that if𝑝=\\nsoftmax(𝑠)for some vector𝑠and 𝑝, then with output gradient𝑑𝑝, the input gradient𝑑𝑠= (diag(𝑝)−𝑝𝑝⊤)𝑑𝑝.\\nStandard attention implementations materialize the matricesS and P to HBM, which takes𝑂(𝑁2)\\nmemory. Often𝑁 ≫𝑑 (typically 𝑁 is on the order of 1k–8k and𝑑 is around 64–128). The standard attention\\nimplementation (1) calls the matrix multiply (GEMM) subroutine to multiplyS = QK⊤, writes the result to\\nHBM, then (2) loads§from HBM to compute softmax and write the resultP to HBM, and finally (3) calls\\nGEMM to getO = PV. As most of the operations are bounded by memory bandwidth, the large number of\\nmemory accesses translates to slow wall-clock time. Moreover, the required memory is𝑂(𝑁2)due to having\\nto materializeS and P. Moreover, one has to saveP ∈R𝑁×𝑁 for the backward pass to compute the gradients.\\n2.3 FlashAttention\\nTo speed up attention on hardware accelerators such as GPU, [5] proposes an algorithm to reduce the memory\\nreads/writes while maintaining the same output (without approximation).\\n2.3.1 Forward pass\\nFlashAttention applies the classical technique of tiling to reduce memory IOs, by (1) loading blocks of\\ninputs from HBM to SRAM, (2) computing attention with respect to that block, and then (3) updating the\\noutput without writing the large intermediate matricesS and P to HBM. As the softmax couples entire rows\\nor blocks of row, online softmax [11, 13] can split the attention computation into blocks, and rescale the\\noutput of each block to finally get the right result (with no approximation). By significantly reducing the\\namount of memory reads/writes,FlashAttention yields 2-4×wall-clock speedup over optimized baseline\\nattention implementations.\\nWe describe the online softmax technique [11] and how it is used in attention [13]. For simplicity, consider\\njust one row block of the attention matrixS, of the form\\n\\x02\\nS(1) S(2)\\x03\\nfor some matricesS(1),S(2) ∈R𝐵𝑟 ×𝐵𝑐 ,\\nwhere 𝐵𝑟 and 𝐵𝑐 are the row and column block sizes. We want to compute softmax of this row block and\\nmultiply with the value, of the form\\n\\x14V(1)\\nV(2)\\n\\x15\\nfor some matricesV(1),V(2) ∈R𝐵𝑐 ×𝑑. Standard softmax would\\n2For clarity of exposition, we omit the scaling ofQK⊤ (typically by1/d), and optionally elementwise masking onS and/or\\ndropout applied toP\\n3'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 3, 'page_label': '4'}, page_content='compute:\\n𝑚= max(rowmax(S(1)),rowmax(S(2)))∈ R𝐵𝑟\\nℓ= rowsum(𝑒S(1)−𝑚)+rowsum(𝑒S(2)−𝑚)∈ R𝐵𝑟\\nP =\\n\\x02\\nP(1) P(2)\\x03\\n= diag(ℓ)−1\\nh\\n𝑒S(1)−𝑚 𝑒S(2)−𝑚\\ni\\n∈R𝐵𝑟 ×2𝐵𝑐\\nO =\\n\\x02\\nP(1) P(2)\\x03 \\x14V(1)\\nV(2)\\n\\x15\\n= diag(ℓ)−1𝑒S(1)−𝑚V(1)+𝑒S(2)−𝑚V(2) ∈R𝐵𝑟 ×𝑑.\\nOnline softmax instead computes “local” softmax with respect to each block and rescale to get the right\\noutput at the end:\\n𝑚(1) = rowmax(S(1))∈ R𝐵𝑟\\nℓ(1) = rowsum(𝑒S(1)−𝑚(1)\\n)∈ R𝐵𝑟\\n˜P(1) = diag(ℓ(1))−1𝑒S(1)−𝑚(1)\\n∈R𝐵𝑟 ×𝐵𝑐\\nO(1) = ˜P(1)V(1) = diag(ℓ(1))−1𝑒S(1)−𝑚(1)\\nV(1) ∈R𝐵𝑟 ×𝑑\\n𝑚(2) = max(𝑚(1),rowmax(S(2)))= 𝑚\\nℓ(2) = 𝑒𝑚(1)−𝑚(2)\\nℓ(1)+rowsum(𝑒S(2)−𝑚(2)\\n)= rowsum(𝑒S(1)−𝑚)+rowsum(𝑒S(2)−𝑚)= ℓ\\n˜P(2) = diag(ℓ(2))−1𝑒S(2)−𝑚(2)\\nO(2) = diag(ℓ(1)/ℓ(2))−1O(1)+˜P(2)V(2) = diag(ℓ(2))−1𝑒𝑠(1)−𝑚V(1)+diag(ℓ(2))−1𝑒𝑠(2)−𝑚V(2) = O.\\nWeshowhow FlashAttentionusesonlinesoftmaxtoenabletiling(Fig.1)toreducememoryreads/writes.\\nFigure 1: Diagram of howFlashAttention forward pass is performed, when the keyK is partitioned into\\ntwo blocks and the valueV is also partitioned into two blocks. By computing attention with respect to\\neach block and rescaling the output, we get the right answer at the end, while avoiding expensive memory\\nreads/writes of the intermediate matricesS and P. We simplify the diagram, omitting the step in softmax\\nthat subtracts each element by the row-wise max.\\n4'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 4, 'page_label': '5'}, page_content='2.3.2 Backward pass\\nIn the backward pass, by re-computing the values of the attention matricesS and P once blocks of inputs\\nQ,K,V are already loaded to SRAM,FlashAttention avoids having to store large intermediate values. By\\nnot having to save the large matricesS and P of size𝑁×𝑁, FlashAttention yields 10-20×memory saving\\ndepending on sequence length (memory required in linear in sequence length𝑁 instead of quadratic). The\\nbackward pass also achieves 2-4×wall-clock speedup due to reduce memory reads/writes.\\nThe backward pass applies tiling to the equations in Section 2.2. Though the backward pass is simpler\\nthan the forward pass conceptually (there is no softmax rescaling), the implementation is significantly more\\ninvolved. This is because there are more values to be kept in SRAM to perform 5 matrix multiples in the\\nbackward pass, compared to just 2 matrix multiples in the forward pass.\\n3 FlashAttention-2: Algorithm, Parallelism, and Work Partition-\\ning\\nWe describe theFlashAttention-2 algorithm, which includes several tweaks toFlashAttention to reduce\\nthe number of non-matmul FLOPs. We then describe how to parallelize the computation on different thread\\nblocks to make full use the GPU resources. Finally we describe we partition the work between different warps\\nwithin one thread block to reduce the amount of shared memory access. These improvements lead to 2-3×\\nspeedup as validated in Section 4.\\n3.1 Algorithm\\nWe tweak the algorithm fromFlashAttention to reduce the number of non-matmul FLOPs. This is\\nbecause modern GPUs have specialized compute units (e.g., Tensor Cores on Nvidia GPUs) that makes\\nmatmul much faster. As an example, the A100 GPU has a max theoretical throughput of 312 TFLOPs/s of\\nFP16/BF16 matmul, but only 19.5 TFLOPs/s of non-matmul FP32. Another way to think about this is that\\neach non-matmul FLOP is 16×more expensive than a matmul FLOP. To maintain high throughput (e.g.,\\nmore than 50% of the maximum theoretical TFLOPs/s), we want to spend as much time on matmul FLOPs\\nas possible.\\n3.1.1 Forward pass\\nWe revisit the online softmax trick as shown in Section 2.3 and make two minor tweaks to reduce non-matmul\\nFLOPs:\\n1. We do not have to rescale both terms of the output update bydiag(ℓ(2))−1:\\nO(2) = diag(ℓ(1)/ℓ(2))−1O(1)+diag(ℓ(2))−1𝑒S(2)−𝑚(2)\\nV(2).\\nWe can instead maintain an “un-scaled” version ofO(2) and keep around the statisticsℓ(2):\\n˜O(2) = diag(ℓ(1))−1O(1)+𝑒S(2)−𝑚(2)\\nV(2).\\nOnly at the every end of the loop do we scale the final˜O(last) by diag(ℓ(last))−1 to get the right output.\\n2. We do not have to save both the max𝑚(𝑗) and the sum of exponentialsℓ(𝑗) for the backward pass. We\\nonly need to store the logsumexp𝐿(𝑗) = 𝑚(𝑗)+log(ℓ(𝑗)).\\n5'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 5, 'page_label': '6'}, page_content='In the simple case of 2 blocks in Section 2.3, the online softmax trick now becomes:\\n𝑚(1) = rowmax(S(1))∈ R𝐵𝑟\\nℓ(1) = rowsum(𝑒S(1)−𝑚(1)\\n)∈ R𝐵𝑟\\n˜O(1) = 𝑒S(1)−𝑚(1)\\nV(1) ∈R𝐵𝑟 ×𝑑\\n𝑚(2) = max(𝑚(1),rowmax(S(2)))= 𝑚\\nℓ(2) = 𝑒𝑚(1)−𝑚(2)\\nℓ(1)+rowsum(𝑒S(2)−𝑚(2)\\n)= rowsum(𝑒S(1)−𝑚)+rowsum(𝑒S(2)−𝑚)= ℓ\\n˜P(2) = diag(ℓ(2))−1𝑒S(2)−𝑚(2)\\n˜O(2) = diag(𝑒𝑚(1)−𝑚(2)\\n)−1 ˜O(1)+𝑒S(2)−𝑚(2)\\nV(2) = 𝑒𝑠(1)−𝑚V(1)+𝑒𝑠(2)−𝑚V(2)\\nO(2) = diag(ℓ(2))−1 ˜O(2) = O.\\nWe describe the fullFlashAttention-2 forward pass in Algorithm 1.\\nAlgorithm 1FlashAttention-2 forward pass\\nRequire: Matrices Q,K,V ∈R𝑁×𝑑 in HBM, block sizes𝐵𝑐, 𝐵𝑟.\\n1: Divide Q into 𝑇𝑟 =\\nl\\n𝑁\\n𝐵𝑟\\nm\\nblocks Q1,..., Q𝑇𝑟 of size𝐵𝑟 ×𝑑 each, and divideK,V in to𝑇𝑐 =\\nl\\n𝑁\\n𝐵𝑐\\nm\\nblocks\\nK1,..., K𝑇𝑐 and V1,..., V𝑇𝑐 , of size𝐵𝑐 ×𝑑 each.\\n2: Divide the outputO ∈R𝑁×𝑑 into 𝑇𝑟 blocks O𝑖,..., O𝑇𝑟 of size𝐵𝑟 ×𝑑 each, and divide the logsumexp𝐿\\ninto 𝑇𝑟 blocks 𝐿𝑖,...,𝐿 𝑇𝑟 of size𝐵𝑟 each.\\n3: for 1 ≤𝑖 ≤𝑇𝑟 do\\n4: Load Q𝑖 from HBM to on-chip SRAM.\\n5: On chip, initializeO(0)\\n𝑖 = (0)𝐵𝑟 ×𝑑 ∈R𝐵𝑟 ×𝑑,ℓ(0)\\n𝑖 = (0)𝐵𝑟 ∈R𝐵𝑟 ,𝑚(0)\\n𝑖 = (−∞)𝐵𝑟 ∈R𝐵𝑟 .\\n6: for 1 ≤𝑗 ≤𝑇𝑐 do\\n7: Load K𝑗,V𝑗 from HBM to on-chip SRAM.\\n8: On chip, computeS(𝑗)\\n𝑖 = Q𝑖K𝑇\\n𝑗 ∈R𝐵𝑟 ×𝐵𝑐 .\\n9: On chip, compute 𝑚(𝑗)\\n𝑖 = max(𝑚(𝑗−1)\\n𝑖 ,rowmax(S(𝑗)\\n𝑖 )) ∈R𝐵𝑟 , ˜P(𝑗)\\n𝑖 = exp(S(𝑗)\\n𝑖 −𝑚(𝑗)\\n𝑖 ) ∈R𝐵𝑟 ×𝐵𝑐\\n(pointwise), ℓ(𝑗)\\n𝑖 = 𝑒𝑚𝑗−1\\n𝑖 −𝑚(𝑗)\\n𝑖 ℓ(𝑗−1)\\n𝑖 +rowsum(˜P(𝑗)\\n𝑖 )∈ R𝐵𝑟 .\\n10: On chip, computeO(𝑗)\\n𝑖 = diag(𝑒𝑚(𝑗−1)\\n𝑖 −𝑚(𝑗)\\n𝑖 )−1O(𝑗−1)\\n𝑖 +˜P(𝑗)\\n𝑖 V𝑗.\\n11: end for\\n12: On chip, computeO𝑖 = diag(ℓ(𝑇𝑐 )\\n𝑖 )−1O(𝑇𝑐 )\\n𝑖 .\\n13: On chip, compute𝐿𝑖 = 𝑚(𝑇𝑐 )\\n𝑖 +log(ℓ(𝑇𝑐 )\\n𝑖 ).\\n14: Write O𝑖 to HBM as the𝑖-th block ofO.\\n15: Write 𝐿𝑖 to HBM as the𝑖-th block of𝐿.\\n16: end for\\n17: Return the outputO and the logsumexp𝐿.\\nCausal masking. One common use case of attention is in auto-regressive language modeling, where we\\nneed to apply a causal mask to the attention matrixS (i.e., any entryS𝑖𝑗 with 𝑗 >𝑖 is set to−∞).\\n1. As FlashAttention and FlashAttention-2 already operate by blocks, for any blocks where all\\nthe column indices are more than the row indices (approximately half of the blocks for large sequence\\nlength), we can skip the computation of that block. This leads to around 1.7-1.8×speedup compared\\nto attention without the causal mask.\\n2. We do not need to apply the causal mask for blocks whose row indices are guaranteed to be strictly less\\nthan the column indices. This means that for each row, we only need apply causal mask to 1 block\\n(assuming square block).\\n6'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 6, 'page_label': '7'}, page_content='Correctness, runtime, and memory requirement.As withFlashAttention, Algorithm 1 returns\\nthe correct outputO = softmax(QK⊤)V (with no approximation), using𝑂(𝑁2𝑑)FLOPs and requires𝑂(𝑁)\\nadditional memory beyond inputs and output (to store the logsumexp𝐿). The proof is almost the same as\\nthe proof of Dao et al. [5, Theorem 1], so we omit it here.\\n3.1.2 Backward pass\\nThe backward pass ofFlashAttention-2 is almost the same as that ofFlashAttention. We make a\\nminor tweak to only use the row-wise logsumexp𝐿 instead of both the row-wise max and row-wise sum of\\nexponentials in the softmax. We include the backward pass description in Algorithm 2 for completeness.\\nAlgorithm 2FlashAttention-2 Backward Pass\\nRequire: Matrices Q,K,V,O,dO ∈R𝑁×𝑑 in HBM, vector𝐿 ∈R𝑁 in HBM, block sizes𝐵𝑐, 𝐵𝑟.\\n1: Divide Q into 𝑇𝑟 =\\nl\\n𝑁\\n𝐵𝑟\\nm\\nblocks Q1,..., Q𝑇𝑟 of size𝐵𝑟 ×𝑑 each, and divideK,V in to𝑇𝑐 =\\nl\\n𝑁\\n𝐵𝑐\\nm\\nblocks\\nK1,..., K𝑇𝑐 and V1,..., V𝑇𝑐 , of size𝐵𝑐 ×𝑑 each.\\n2: Divide O into 𝑇𝑟 blocks O𝑖,..., O𝑇𝑟 of size𝐵𝑟 ×𝑑 each, dividedO into 𝑇𝑟 blocks dO𝑖,..., dO𝑇𝑟 of size\\n𝐵𝑟 ×𝑑 each, and divide𝐿 into 𝑇𝑟 blocks 𝐿𝑖,...,𝐿 𝑇𝑟 of size𝐵𝑟 each.\\n3: Initialize dQ = (0)𝑁×𝑑 in HBM and divide it into𝑇𝑟 blocks dQ1,..., dQ𝑇𝑟 of size 𝐵𝑟 ×𝑑 each. Divide\\ndK,dV ∈R𝑁×𝑑 in to𝑇𝑐 blocks dK1,..., dK𝑇𝑐 and dV1,..., dV𝑇𝑐 , of size𝐵𝑐 ×𝑑 each.\\n4: Compute 𝐷 = rowsum(dO ◦O)∈ R𝑑 (pointwise multiply), write𝐷 to HBM and divide it into𝑇𝑟 blocks\\n𝐷1,...,𝐷 𝑇𝑟 of size𝐵𝑟 each.\\n5: for 1 ≤𝑗 ≤𝑇𝑐 do\\n6: Load K𝑗,V𝑗 from HBM to on-chip SRAM.\\n7: Initialize dK𝑗 = (0)𝐵𝑐 ×𝑑,dV𝑗 = (0)𝐵𝑐 ×𝑑 on SRAM.\\n8: for 1 ≤𝑖 ≤𝑇𝑟 do\\n9: Load Q𝑖,O𝑖,dO𝑖,dQ𝑖,𝐿𝑖,𝐷𝑖 from HBM to on-chip SRAM.\\n10: On chip, computeS(𝑗)\\n𝑖 = Q𝑖K𝑇\\n𝑗 ∈R𝐵𝑟 ×𝐵𝑐 .\\n11: On chip, computeP(𝑗)\\n𝑖 = exp(S𝑖𝑗 −𝐿𝑖)∈ R𝐵𝑟 ×𝐵𝑐 .\\n12: On chip, computedV𝑗 ←dV𝑗 +(P(𝑗)\\n𝑖 )⊤dO𝑖 ∈R𝐵𝑐 ×𝑑.\\n13: On chip, computedP(𝑗)\\n𝑖 = dO𝑖V⊤\\n𝑗 ∈R𝐵𝑟 ×𝐵𝑐 .\\n14: On chip, computedS(𝑗)\\n𝑖 = P(𝑗)\\n𝑖 ◦(dP(𝑗)\\n𝑖 −𝐷𝑖)∈ R𝐵𝑟 ×𝐵𝑐 .\\n15: Load dQ𝑖 from HBM to SRAM, then on chip, updatedQ𝑖 ←dQ𝑖 +dS(𝑗)\\n𝑖 K𝑗 ∈R𝐵𝑟 ×𝑑, and write back\\nto HBM.\\n16: On chip, computedK𝑗 ←dK𝑗 +dS(𝑗)\\n𝑖\\n⊤\\nQ𝑖 ∈R𝐵𝑐 ×𝑑.\\n17: end for\\n18: Write dK𝑗,dV𝑗 to HBM.\\n19: end for\\n20: Return dQ,dK,dV.\\nMulti-query attention and grouped-query attention.Multi-query attention (MQA) [15] and grouped-\\nquery attention (GQA) [1] are variants of attention where multiple heads of query attend to the same head of\\nkey and value, in order to reduce the size of KV cache during inference. Instead of having to duplicate the\\nkey and value heads for the computation, we implicitly manipulate the indices into the head to perform the\\nsame computation. In the backward pass, we need to sum the gradientsdK and dV across different heads\\nthat were implicitly duplicated.\\n3.2 Parallelism\\nThe first version ofFlashAttention parallelizes over batch size and number of heads. We use 1 thread\\nblock to process one attention head, and there are overallbatch size·number of headsthread blocks. Each\\nthread block is scheduled to run on a streaming multiprocessor (SM), and there are 108 of these SMs on\\n7'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 7, 'page_label': '8'}, page_content='an A100 GPU for example. This scheduling is efficient when this number is large (say≥80), since we can\\neffectively use almost all of the compute resources on the GPU.\\nIn the case of long sequences (which usually means small batch sizes or small number of heads), to make\\nbetter use of the multiprocessors on the GPU, we now additionally parallelize over the sequence length\\ndimension. This results in significant speedup for this regime.\\nForward pass. We see that the outer loop (over sequence length) is embarrassingly parallel, and we\\nschedule them on different thread blocks that do not need to communicate with each other. We also parallelize\\nover the batch dimension and number of heads dimension, as done inFlashAttention. The increased\\nparallelism over sequence length helps improve occupancy (fraction of GPU resources being used) when the\\nbatch size and number of heads are small, leading to speedup in this case.\\nThese ideas of swapping the order of the loop (outer loop over row blocks and inner loop over column\\nblocks, instead of the other way round in the originalFlashAttention paper), as well as parallelizing\\nover the sequence length dimension were first suggested and implemented by Phil Tillet in the Triton [17]\\nimplementation.3\\nBackward pass. Notice that the only shared computation between different column blocks is in updatedQ\\nin Algorithm 2, where we need to loaddQ𝑖 from HBM to SRAM, then on chip, updatedQ𝑖 ←dQ𝑖 +dS(𝑗)\\n𝑖 K𝑗,\\nand write back to HBM. We thus parallelize over the sequence length dimension as well, and schedule 1\\nthread block for each column block of the backward pass. We use atomic adds to communicate between\\ndifferent thread blocks to updatedQ.\\nWe describe the parallelization scheme in Fig. 2.\\nFigure 2: In the forward pass (left), we parallelize the workers (thread blocks) where each worker takes care\\nof a block of rows of the attention matrix. In the backward pass (right), each worker takes care of a block of\\ncolumns of the attention matrix.\\n3https://github.com/openai/triton/blob/main/python/tutorials/06-fused-attention.py\\n8'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 8, 'page_label': '9'}, page_content='3.3 Work Partitioning Between Warps\\nAs Section 3.2 describe how we schedule thread blocks, even within each thread block, we also have to decide\\nhow to partition the work between different warps. We typically use 4 or 8 warps per thread block, and the\\npartitioning is described in Fig. 3.\\nForward pass. For each block,FlashAttention splits K and V across 4 warps while keepingQ accessible\\nby all warps. Each warp multiplies to get a slice ofQK⊤, then they need to multiply with a slice ofV and\\ncommunicate to add up the result. This is referred to as the “split-K” scheme. However, this is inefficient\\nsince all warps need to write their intermediate results out to shared memory, synchronize, then add up the\\nintermediate results. These shared memory reads/writes slow down the forward pass inFlashAttention.\\nIn FlashAttention-2, we instead splitQ across 4 warps while keepingK and V accessible by all warps.\\nAfter each warp performs matrix multiply to get a slice ofQK⊤, they just need to multiply with their shared\\nslice ofV to get their corresponding slice of the output. There is no need for communication between warps.\\nThe reduction in shared memory reads/writes yields speedup (Section 4).\\n(a) FlashAttention\\n (b) FlashAttention-2\\nFigure 3: Work partitioning between different warps in the forward pass\\nBackward pass. Similarly for the backward pass, we choose to partition the warps to avoid the “split-K”\\nscheme. However, it still requires some synchronization due to the more complicated dependency between all\\nthe different inputs and gradientsQ,K,V,O,dO,dQ,dK,dV. Nevertheless, avoiding “split-K” reduces shared\\nmemory reads/writes and again yields speedup (Section 4).\\nTuning block sizes Increasing block sizes generally reduces shared memory loads/stores, but increases\\nthe number of registers required and the total amount of shared memory. Past a certain block size, register\\nspilling causes significant slowdown, or the amount of shared memory required is larger than what the GPU\\nhas available, and the kernel cannot run at all. Typically we choose blocks of size{64,128}×{ 64,128},\\ndepending on the head dimension𝑑 and the device shared memory size.\\nWe manually tune for each head dimensions since there are essentially only 4 choices for block sizes, but\\nthis could benefit from auto-tuning to avoid this manual labor. We leave this to future work.\\n4 Empirical Validation\\nWe evaluate the impact of usingFlashAttention-2 to train Transformer models.\\n• Benchmarking attention.We measure the runtime ofFlashAttention-2 across different sequence\\nlengthsandcompareittoastandardimplementationinPyTorch, FlashAttention, andFlashAttention\\nin Triton. We confirm that FlashAttention-2 is 1.7-3.0×faster than FlashAttention, 1.3-2.5×\\nfaster than FlashAttention in Triton, and 3-10×faster than a standard attention implementation.\\n9'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 9, 'page_label': '10'}, page_content='FlashAttention-2 reaches up to 230 TFLOPs/s, 73% of the theoretical maximum TFLOPs/s on A100\\nGPUs.\\n• End-to-end training speedWhen used end-to-end to train GPT-style models of size 1.3B and 2.7B on\\nsequence lengths either 2k or 8k,FlashAttention-2 yields up to 1.3×speedup compared toFlashAt-\\ntention and 2.8×speedup compared to a baseline withoutFlashAttention. FlashAttention-2\\nreaches up to 225 TFLOPs/s (72% model FLOPs utilization) per A100 GPU.\\n4.1 Benchmarking Attention\\nWe measure the runtime of different attention methods on an A100 80GB SXM4 GPU for different settings\\n(without / with causal mask, head dimension 64 or 128). We report the results in Fig. 4, Fig. 5 and Fig. 6,\\nshowing thatFlashAttention-2 is around 2×faster thanFlashAttention and FlashAttention in\\nxformers (the “cutlass” implementation).FlashAttention-2 is around 1.3-1.5×faster thanFlashAtten-\\ntion in Triton in the forward pass and around 2×faster in the backward pass. Compared to a standard\\nattention implementation in PyTorch,FlashAttention-2 can be up to 10×faster.\\nBenchmark setting: we vary the sequence length from 512, 1k, ..., 16k, and set batch size so that the total\\nnumber of tokens is 16k. We set hidden dimension to 2048, and head dimension to be either 64 or 128 (i.e.,\\n32 heads or 16 heads). To calculate the FLOPs of the forward pass, we use:\\n4 ·seqlen2 ·head dimension·number of heads.\\nWith causal mask, we divide this number by 2 to account for the fact that approximately only half of the\\nentries are calculated. To get the FLOPs of the backward pass, we multiply the forward pass FLOPs by 2.5\\n(since there are 2 matmuls in the forward pass and 5 matmuls in the backward pass, due to recomputation).\\n512 1k 2k 4k 8k 16k\\nSequence length\\n50\\n100\\n150\\n200Speed (TFLOPs/s)\\n36 40 43 45 46\\nOOM\\n91 92\\n104 108 110 110\\n68 73 76 77 75 75\\n90\\n102 98 98 100 100\\n132\\n153\\n162\\n171 175 176\\nAttention forward + backward speed (A100 80GB SXM4)\\nPytorch\\nFlashAttention\\nxformers\\nFlashAttention Triton\\nFlashAttention-2\\n(a) Without causal mask, head dimension 64\\n512 1k 2k 4k 8k 16k\\nSequence length\\n50\\n100\\n150\\n200Speed (TFLOPs/s)53\\n67\\n76 79 86\\nOOM\\n76 72 76 80 82 8383\\n91 95 96 97 98\\n78 85 90 93 95 95\\n151\\n173\\n187\\n196 201 203\\nAttention forward + backward speed (A100 80GB SXM4)\\nPytorch\\nFlashAttention\\nxformers\\nFlashAttention Triton\\nFlashAttention-2 (b) Without causal mask, head dimension 128\\n512 1k 2k 4k 8k 16k\\nSequence length\\n50\\n100\\n150\\n200Speed (TFLOPs/s)\\n15 16 17 18 18\\nOOM\\n58\\n70\\n77\\n87 92 97\\n51\\n60 66 68 69 67\\n59\\n75 79 76 79 80\\n88\\n119\\n140\\n156\\n165 171\\nAttention forward + backward speed (A100 80GB SXM4)\\nPytorch\\nFlashAttention\\nxformers\\nFlashAttention Triton\\nFlashAttention-2\\n(c) With causal mask, head dimension 64\\n512 1k 2k 4k 8k 16k\\nSequence length\\n50\\n100\\n150\\n200Speed (TFLOPs/s)\\n23 28 32 32 34\\nOOM\\n55 62\\n69 76 80 83\\n58\\n72\\n82 87 91 92\\n50\\n61\\n68 74 78 80\\n99\\n133\\n155\\n173\\n182 189\\nAttention forward + backward speed (A100 80GB SXM4)\\nPytorch\\nFlashAttention\\nxformers\\nFlashAttention Triton\\nFlashAttention-2 (d) With causal mask, head dimension 128\\nFigure 4: Attention forward + backward speed on A100 GPU\\n10'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 10, 'page_label': '11'}, page_content='512 1k 2k 4k 8k 16k\\nSequence length\\n50\\n100\\n150\\n200Speed (TFLOPs/s)\\n29 34 35 37 37\\nOOM\\n91 96 99 104 104 104\\n89 94 97 99 98 98\\n128\\n141\\n149 152 152 155\\n178\\n191 193 192 192 192\\nAttention forward speed (A100 80GB SXM4)\\nPytorch\\nFlashAttention\\nxformers\\nFlashAttention Triton\\nFlashAttention-2\\n(a) Without causal mask, head dimension 64\\n512 1k 2k 4k 8k 16k\\nSequence length\\n50\\n100\\n150\\n200Speed (TFLOPs/s)42\\n56 60 63 67\\nOOM\\n69 66 71 71 72 73\\n107\\n115 120 122 122 122127\\n140\\n152 157 160 163\\n209\\n224 227 222 224 223\\nAttention forward speed (A100 80GB SXM4)\\nPytorch\\nFlashAttention\\nxformers\\nFlashAttention Triton\\nFlashAttention-2 (b) Without causal mask, head dimension 128\\n512 1k 2k 4k 8k 16k\\nSequence length\\n50\\n100\\n150\\n200Speed (TFLOPs/s)\\n10 10 10 10 10\\nOOM\\n56\\n70\\n81\\n89 91 94\\n71\\n82 89 92 94 95\\n78\\n99\\n112\\n131 137 143\\n115\\n146\\n167\\n177 181 183\\nAttention forward speed (A100 80GB SXM4)\\nPytorch\\nFlashAttention\\nxformers\\nFlashAttention Triton\\nFlashAttention-2\\n(c) With causal mask, head dimension 64\\n512 1k 2k 4k 8k 16k\\nSequence length\\n50\\n100\\n150\\n200Speed (TFLOPs/s)\\n15 18 19 19 19\\nOOM\\n49\\n59 65 68 70 71\\n79\\n95\\n107 112 115 117\\n89\\n108\\n126\\n133\\n141\\n148\\n132\\n168\\n187\\n198 200 197\\nAttention forward speed (A100 80GB SXM4)\\nPytorch\\nFlashAttention\\nxformers\\nFlashAttention Triton\\nFlashAttention-2 (d) With causal mask, head dimension 128\\nFigure 5: Attention forward speed on A100 GPU\\nJust running the same implementation on H100 GPUs (using no special instructions to make use of new\\nfeatures such as TMA and 4th-gen Tensor Cores), we obtain up to 335 TFLOPs/s (Fig. 7). We expect that by\\nusing new instructions, we can obtain another 1.5x-2x speedup on H100 GPUs. We leave that to future work.\\n4.2 End-to-end Performance\\nWe measure the training throughput of GPT-style models with either 1.3B or 2.7B parameters, on 8×A100\\n80GB SXM. As shown in Table 1,FlashAttention-2 yields 2.8×speedup compared to a baseline without\\nFlashAttention and 1.3×speedup compared toFlashAttention-2, reaching up to 225 TFLOPs/s per A100\\nGPU.\\nNote that we calculate the FLOPs by the formula, following Megatron-LM [16] (and many other papers\\nand libraries):\\n6 ·seqlen·number of params+12 ·number of layers·hidden dim·seqlen2.\\nThe first term accounts for the FLOPs due to weight–input multiplication, and the second term accounts for\\nthe FLOPs due to attention. However, one can argue that the second term should be halved, as with causal\\nmask we only need to compute approximately half the number of elements in attention. We choose to follow\\nthe formula from the literature (without dividing the attention FLOPs by 2) for consistency.\\n5 Discussion and Future Directions\\nFlashAttention-2 is 2×faster thanFlashAttention, which means that we can train models with 16k\\nlonger context for the same price as previously training a 8k context model. We are excited about how this can\\n11'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 11, 'page_label': '12'}, page_content='512 1k 2k 4k 8k 16k\\nSequence length\\n50\\n100\\n150\\n200Speed (TFLOPs/s)\\n39 43 48 49 51\\nOOM\\n91 90\\n106 109 112 113\\n62 67 70 70 69 68\\n81\\n92 87 86 87 88\\n120\\n141\\n152\\n163 169 170\\nAttention backward speed (A100 80GB SXM4)\\nPytorch\\nFlashAttention\\nxformers\\nFlashAttention Triton\\nFlashAttention-2\\n(a) Without causal mask, head dimension 64\\n512 1k 2k 4k 8k 16k\\nSequence length\\n50\\n100\\n150\\n200Speed (TFLOPs/s)\\n59\\n73\\n86 88\\n97\\nOOM\\n78 75 79 84 86 88\\n76\\n84 88 89 90 91\\n68 74 77 80 82 81\\n136\\n159\\n175\\n187 193 196\\nAttention backward speed (A100 80GB SXM4)\\nPytorch\\nFlashAttention\\nxformers\\nFlashAttention Triton\\nFlashAttention-2 (b) Without causal mask, head dimension 128\\n512 1k 2k 4k 8k 16k\\nSequence length\\n50\\n100\\n150\\n200Speed (TFLOPs/s)\\n19 21 24 25 26\\nOOM\\n58\\n70 76\\n85\\n93 98\\n46\\n54 60 62 62 6053\\n68 71 65 67 68\\n81\\n111\\n131\\n149\\n160 166\\nAttention backward speed (A100 80GB SXM4)\\nPytorch\\nFlashAttention\\nxformers\\nFlashAttention Triton\\nFlashAttention-2\\n(c) With causal mask, head dimension 64\\n512 1k 2k 4k 8k 16k\\nSequence length\\n50\\n100\\n150\\n200Speed (TFLOPs/s)\\n30\\n37 43 45 49\\nOOM\\n59 63\\n71\\n80 86 89\\n53\\n65\\n75 80 84 84\\n43\\n52 58 63 66 67\\n90\\n122\\n145\\n165\\n176\\n186\\nAttention backward speed (A100 80GB SXM4)\\nPytorch\\nFlashAttention\\nxformers\\nFlashAttention Triton\\nFlashAttention-2 (d) With causal mask, head dimension 128\\nFigure 6: Attention backward speed on A100 GPU\\nTable 1: Training speed (TFLOPs/s/GPU) of GPT-style models on 8×A100 GPUs. FlashAttention-2\\nreaches up to 225 TFLOPs/s (72% model FLOPs utilization). We compare against a baseline running without\\nFlashAttention.\\nModel Without FlashAttention FlashAttention FlashAttention-2\\nGPT3-1.3B 2k context 142 TFLOPs/s 189 TFLOPs/s 196 TFLOPs/s\\nGPT3-1.3B 8k context 72 TFLOPS/s 170 TFLOPs/s 220 TFLOPs/s\\nGPT3-2.7B 2k context 149 TFLOPs/s 189 TFLOPs/s 205 TFLOPs/s\\nGPT3-2.7B 8k context 80 TFLOPs/s 175 TFLOPs/s 225 TFLOPs/s\\nbe used to understand long books and reports, high resolution images, audio and video.FlashAttention-2\\nwill also speed up training, finetuning, and inference of existing models.\\nIn the near future, we plan to collaborate with researchers and engineers to make FlashAttention widely\\napplicable in different kinds of devices (e.g., H100 GPUs, AMD GPUs), as well as new data types such as\\nFP8. As an immediate next step, we plan to optimize FlashAttention-2 for H100 GPUs to use new hardware\\nfeatures (TMA, 4th-gen Tensor Cores, fp8). Combining the low-level optimizations in FlashAttention-2 with\\nhigh-level algorithmic changes (e.g., local, dilated, block-sparse attention) could allow us to train AI models\\nwith much longer context. We are also excited to work with compiler researchers to make these optimization\\ntechniques easily programmable.\\n12'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 12, 'page_label': '13'}, page_content='512 1k 2k 4k 8k 16k\\nSequence length\\n100\\n200\\n300Speed (TFLOPs/s)\\n62 72 81 86 87\\nOOM\\n157 159 161 161 166 168\\n215\\n254\\n274\\n288 294 296\\nAttention forward + backward speed (H100 80GB SXM5)\\nPytorch\\nFlashAttention\\nFlashAttention-2\\n(a) Without causal mask, head dimension 64\\n512 1k 2k 4k 8k 16k\\nSequence length\\n100\\n200\\n300Speed (TFLOPs/s)\\n93\\n120\\n145\\n160 167\\nOOM\\n127 127 128 131 137 139\\n248\\n294\\n320 326 335 338\\nAttention forward + backward speed (H100 80GB SXM5)\\nPytorch\\nFlashAttention\\nFlashAttention-2 (b) Without causal mask, head dimension 128\\n512 1k 2k 4k 8k 16k\\nSequence length\\n100\\n200\\n300Speed (TFLOPs/s)\\n26 29 31 32 32\\nOOM\\n104\\n123\\n136 138 149 156\\n141\\n192\\n232\\n257\\n273 284\\nAttention forward + backward speed (H100 80GB SXM5)\\nPytorch\\nFlashAttention\\nFlashAttention-2\\n(c) With causal mask, head dimension 64\\n512 1k 2k 4k 8k 16k\\nSequence length\\n100\\n200\\n300Speed (TFLOPs/s)\\n40 50 57 61 63\\nOOM\\n98 109 108\\n126 135 137\\n163\\n221\\n265\\n294\\n308\\n328\\nAttention forward + backward speed (H100 80GB SXM5)\\nPytorch\\nFlashAttention\\nFlashAttention-2 (d) With causal mask, head dimension 128\\nFigure 7: Attention forward + backward speed on H100 GPU\\nAcknowledgments\\nWethankPhilTilletandDanielHaziza, whohaveimplementedversionsof FlashAttentioninTriton[ 17]and\\nthexformers library[10]. FlashAttention-2wasmotivatedbyexchangeofideasbetweendifferentwaysthat\\nattention could be implemented. We are grateful to the Nvidia CUTLASS team (especially Vijay Thakkar, Cris\\nCecka, Haicheng Wu, and Andrew Kerr) for their CUTLASS library, in particular the CUTLASS 3.x release,\\nwhich provides clean abstractions and powerful building blocks for the implementation ofFlashAttention-2.\\nWe thank Driss Guessous for integratingFlashAttention to PyTorch.FlashAttention-2 has benefited\\nfrom helpful discussions with Phil Wang, Markus Rabe, James Bradbury, Young-Jun Ko, Julien Launay,\\nDaniel Hesslow, Michaël Benesty, Horace He, Ashish Vaswani, and Erich Elsen. Thanks for Stanford CRFM\\nand Stanford NLP for the compute support. We thank Dan Fu and Christopher Ré for their collaboration,\\nconstructive feedback, and constant encouragement on this line of work of designing hardware-efficient\\nalgorithms. We thank Albert Gu and Beidi Chen for their helpful suggestions on early drafts of this technical\\nreport.\\nReferences\\n[1] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit\\nSanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints.arXiv\\npreprint arXiv:2305.13245, 2023.\\n[2] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.arXiv\\npreprint arXiv:2004.05150, 2020.\\n13'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 13, 'page_label': '14'}, page_content='[3] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher Ré. Scatterbrain: Unifying\\nsparse and low-rank attention. InAdvances in Neural Information Processing Systems (NeurIPS), 2021.\\n[4] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane,\\nTamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking\\nattention with performers. InInternational Conference on Learning Representations (ICLR), 2020.\\n[5] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention: Fast and\\nmemory-efficient exact attention with IO-awareness. InAdvances in Neural Information Processing\\nSystems, 2022.\\n[6] Zhe Jia and Peter Van Sandt. Dissecting the Ampere GPU architecture via microbenchmarking. GPU\\nTechnology Conference, 2021.\\n[7] Zhe Jia, Marco Maggioni, Benjamin Staiger, and Daniele P Scarpazza. Dissecting the nvidia Volta GPU\\narchitecture via microbenchmarking.arXiv preprint arXiv:1804.06826, 2018.\\n[8] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are RNNs:\\nFast autoregressive transformers with linear attention. InInternational Conference on Machine Learning,\\npages 5156–5165. PMLR, 2020.\\n[9] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. InThe\\nInternational Conference on Machine Learning (ICML), 2020.\\n[10] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren,\\nMin Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel Haziza. xformers: A modular\\nand hackable transformer modelling library.https://github.com/facebookresearch/xformers, 2022.\\n[11] Maxim Milakov and Natalia Gimelshein. Online normalizer calculation for softmax.arXiv preprint\\narXiv:1805.02867, 2018.\\n[12] OpenAI. Gpt-4 technical report.ArXiv, abs/2303.08774, 2023.\\n[13] Markus N Rabe and Charles Staats. Self-attention does not need 𝑂(𝑛2)memory. arXiv preprint\\narXiv:2112.05682, 2021.\\n[14] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse\\nattention with routing transformers.Transactions of the Association for Computational Linguistics, 9:\\n53–68, 2021.\\n[15] Noam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint\\narXiv:1911.02150, 2019.\\n[16] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro.\\nMegatron-LM: Training multi-billion parameter language models using model parallelism.arXiv preprint\\narXiv:1909.08053, 2019.\\n[17] Philippe Tillet, Hsiang-Tsung Kung, and David Cox. Triton: an intermediate language and compiler for\\ntiled neural network computations. InProceedings of the 3rd ACM SIGPLAN International Workshop\\non Machine Learning and Programming Languages, pages 10–19, 2019.\\n[18] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\\nKaiser, and Illia Polosukhin. Attention is all you need.Advances in neural information processing\\nsystems, 30, 2017.\\n[19] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with\\nlinear complexity.arXiv preprint arXiv:2006.04768, 2020.\\n[20] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago\\nOntanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer\\nsequences. Advances in Neural Information Processing Systems, 33, 2020.\\n14')]"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["from langchain_text_splitters import RecursiveCharacterTextSplitter\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=120)\n","docs = text_splitter.split_documents(documents)"],"metadata":{"id":"53ii7Ect19Qz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["docs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VhtpKgrS3Ika","executionInfo":{"status":"ok","timestamp":1738855456178,"user_tz":-330,"elapsed":1694,"user":{"displayName":"Brue Pl","userId":"14478803105777518815"}},"outputId":"84465187-45b0-4ebd-bef9-f31ccdc749e9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 0, 'page_label': '1'}, page_content='FlashAttention-2:\\nFaster Attention with Better Parallelism and Work Partitioning\\nTri Dao1,2\\n1Department of Computer Science, Princeton University\\n2Department of Computer Science, Stanford University\\ntrid@cs.stanford.edu\\nJuly 18, 2023\\nAbstract\\nScaling Transformers to longer sequence lengths has been a major problem in the last several years,\\npromising to improve performance in language modeling and high-resolution image understanding, as\\nwell as to unlock new applications in code, audio, and video generation. The attention layer is the\\nmain bottleneck in scaling to longer sequences, as its runtime and memory increase quadratically in\\nthe sequence length. FlashAttention [5] exploits the asymmetric GPU memory hierarchy to bring\\nsignificant memory saving (linear instead of quadratic) and runtime speedup (2-4×compared to optimized\\nbaselines), with no approximation. However,FlashAttention is still not nearly as fast as optimized'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 0, 'page_label': '1'}, page_content='baselines), with no approximation. However,FlashAttention is still not nearly as fast as optimized\\nmatrix-multiply (GEMM) operations, reaching only 25-40% of the theoretical maximum FLOPs/s. We\\nobserve that the inefficiency is due to suboptimal work partitioning between different thread blocks and\\nwarps on the GPU, causing either low-occupancy or unnecessary shared memory reads/writes. We propose\\nFlashAttention-2, with better work partitioning to address these issues. In particular, we (1) tweak\\nthe algorithm to reduce the number of non-matmul FLOPs (2) parallelize the attention computation, even\\nfor a single head, across different thread blocks to increase occupancy, and (3) within each thread block,\\ndistribute the work between warps to reduce communication through shared memory. These yield around\\n2×speedup compared toFlashAttention, reaching 50-73% of the theoretical maximum FLOPs/s on'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 0, 'page_label': '1'}, page_content='2×speedup compared toFlashAttention, reaching 50-73% of the theoretical maximum FLOPs/s on\\nA100 and getting close to the efficiency of GEMM operations. We empirically validate that when used\\nend-to-end to train GPT-style models,FlashAttention-2 reaches training speed of up to 225 TFLOPs/s\\nper A100 GPU (72% model FLOPs utilization).1\\n1 Introduction\\nScaling up the context length of Transformers [18] is a challenge, since the attention layer at their heart\\nhas runtime and memory requirements quadratic in the input sequence length. Ideally, we would like to go\\nbeyond the standard 2k sequence length limit to train models to understand books, high resolution images,\\nand long-form videos. Just within the last year, there have been several language models with much longer\\ncontext than before: GPT-4 [12] with context length 32k, MosaicML’s MPT with context length 65k, and\\nAnthropic’s Claude with context length 100k. Emerging use cases such as long document querying and story'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 0, 'page_label': '1'}, page_content='Anthropic’s Claude with context length 100k. Emerging use cases such as long document querying and story\\nwriting have demonstrated a need for models with such long context.\\nTo reduce the computational requirement of attention on such long context, there have been numerous\\nmethods proposed to approximate attention [2, 3, 4, 8, 9, 14, 19, 20]. Though these methods have seen\\nsome use cases, as far as we know, most large-scale training runs still use standard attention. Motivated by\\nthis, Dao et al.[5] proposed to reorder the attention computation and leverages classical techniques (tiling,\\nrecomputation) to significantly speed it up and reduce memory usage from quadratic to linear in sequence\\nlength. This yields 2-4×wall-clock time speedup over optimized baselines, up to 10-20×memory saving,\\n1FlashAttention-2 is available athttps://github.com/Dao-AILab/flash-attention\\n1\\narXiv:2307.08691v1  [cs.LG]  17 Jul 2023'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 1, 'page_label': '2'}, page_content='with no approximation, and as a resultFlashAttention has seen wide adoption in large-scale training and\\ninference of Transformers.\\nHowever, context length increases even more,FlashAttention is still not nearly as efficient as other\\nprimitives such as matrix-multiply (GEMM). In particular, whileFlashAttention is already 2-4×faster\\nthan a standard attention implementation, the forward pass only reaches 30-50% of the theoretical maximum\\nFLOPs/s of the device (Fig. 5), while the backward pass is even more challenging, reaching only 25-35%\\nof maximum throughput on A100 GPU (Fig. 6). In contrast, optimized GEMM can reach up to 80-90% of\\nthe theoretical maximum device throughput. Through careful profiling, we observe thatFlashAttention\\nstill has suboptimal work partitioning between different thread blocks and warps on the GPU, causing either\\nlow-occupancy or unnecessary shared memory reads/writes.\\nBuilding on FlashAttention, we propose FlashAttention-2 with better parallelism and work'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 1, 'page_label': '2'}, page_content='Building on FlashAttention, we propose FlashAttention-2 with better parallelism and work\\npartitioning to address these challenges.\\n1. In Section 3.1, we tweak the algorithms to reduce the number of non-matmul FLOPs while not changing\\nthe output. While the non-matmul FLOPs only account for a small fraction of the total FLOPs, they\\ntake longer to perform as GPUs have specialized units for matrix multiply, and as a result the matmul\\nthroughput can be up to 16×higher than non-matmul throughput. It is thus important to reduce\\nnon-matmul FLOPs and spend as much time as possible doing matmul FLOPs.\\n2. We propose to parallelize both the forward pass and backward pass along the sequence length dimension,\\nin addition to the batch and number of heads dimension. This increases occupancy (utilization of GPU\\nresources) in the case where the sequences are long (and hence batch size is often small).\\n3. Even within one block of attention computation, we partition the work between different warps of a'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 1, 'page_label': '2'}, page_content='3. Even within one block of attention computation, we partition the work between different warps of a\\nthread block to reduce communication and shared memory reads/writes.\\nIn Section 4, we empirically validate thatFlashAttention-2 yields significant speedup compared to\\neven FlashAttention. Benchmarks on different settings (with or without causal mask, different head\\ndimensions) show thatFlashAttention-2 achieves around 2×speedup overFlashAttention, reaching\\nup to 73% of the theoretical max throughput in the forward pass, and up to 63% of the theoretical max\\nthroughput in the backward pass. When used end-to-end to train GPT-style models, we reach training speed\\nof up to 225 TFLOPs/s per A100 GPU.\\n2 Background\\nWe provide some background on the performance characteristics and execution model of GPUs. We also\\ndescribe the standard implementation of attention, as well asFlashAttention.\\n2.1 Hardware characteristics'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 1, 'page_label': '2'}, page_content='describe the standard implementation of attention, as well asFlashAttention.\\n2.1 Hardware characteristics\\nGPU performance characteristics.The GPU consists of compute elements (e.g., floating point arithmetic\\nunits) and a memory hierarchy. Most modern GPUs contain specialized units to accelerate matrix multiply in\\nlow-precision (e.g., Tensor Cores on Nvidia GPUs for FP16/BF16 matrix multiply). The memory hierarchy\\ncomprise of high bandwidth memory (HBM), and on-chip SRAM (aka shared memory). As an example, the\\nA100 GPU has 40-80GB of high bandwidth memory (HBM) with bandwidth 1.5-2.0TB/s and 192KB of\\non-chip SRAM per each of 108 streaming multiprocessors with bandwidth estimated around 19TB/s [6, 7].\\nAs the L2 cache is not directly controllable by the programmer, we focus on the HBM and SRAM for the\\npurpose of this discussion.\\nExecution Model. GPUs have a massive number of threads to execute an operation (called a kernel).'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 1, 'page_label': '2'}, page_content='Execution Model. GPUs have a massive number of threads to execute an operation (called a kernel).\\nThreads are organized into thread blocks, which are scheduled to run on streaming multiprocessors (SMs).\\nWithin each thread blocks, threads are grouped into warps (a group of 32 threads). Threads within a warp\\ncan communicate by fast shuffle instructions or cooperate to perform matrix multiply. Warps within a thread\\nblock can communicate by reading from / writing to shared memory. Each kernel loads inputs from HBM to\\nregisters and SRAM, computes, then writes outputs to HBM.\\n2'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 2, 'page_label': '3'}, page_content='2.2 Standard Attention Implementation\\nGiven input sequencesQ,K,V ∈R𝑁×𝑑 where 𝑁 is the sequence length and𝑑 is the head dimension, we want\\nto compute the attention outputO ∈R𝑁×𝑑:\\nS = QK⊤∈R𝑁×𝑁, P = softmax(S)∈ R𝑁×𝑁, O = PV ∈R𝑁×𝑑,\\nwhere softmax is applied row-wise.2 For multi-head attention (MHA), this same computation is performed in\\nparallel across many heads, and parallel over the batch dimension (number of input sequences in a batch).\\nThe backward pass of attention proceeds as follows. LetdO ∈R𝑁×𝑑 be the gradient ofO with respect to\\nsome loss function. Then by the chain rule (aka backpropagation):\\ndV = P⊤dO ∈R𝑁×𝑑\\ndP = dOV⊤∈R𝑁×𝑁\\ndS = dsoftmax(dP)∈ R𝑁×𝑁\\ndQ = dSK ∈R𝑁×𝑑\\ndK = QdS⊤∈R𝑁×𝑑,\\nwhere dsoftmax is the gradient (backward pass) of softmax applied row-wise. One can work out that if𝑝=\\nsoftmax(𝑠)for some vector𝑠and 𝑝, then with output gradient𝑑𝑝, the input gradient𝑑𝑠= (diag(𝑝)−𝑝𝑝⊤)𝑑𝑝.\\nStandard attention implementations materialize the matricesS and P to HBM, which takes𝑂(𝑁2)'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 2, 'page_label': '3'}, page_content='Standard attention implementations materialize the matricesS and P to HBM, which takes𝑂(𝑁2)\\nmemory. Often𝑁 ≫𝑑 (typically 𝑁 is on the order of 1k–8k and𝑑 is around 64–128). The standard attention\\nimplementation (1) calls the matrix multiply (GEMM) subroutine to multiplyS = QK⊤, writes the result to\\nHBM, then (2) loads§from HBM to compute softmax and write the resultP to HBM, and finally (3) calls\\nGEMM to getO = PV. As most of the operations are bounded by memory bandwidth, the large number of\\nmemory accesses translates to slow wall-clock time. Moreover, the required memory is𝑂(𝑁2)due to having\\nto materializeS and P. Moreover, one has to saveP ∈R𝑁×𝑁 for the backward pass to compute the gradients.\\n2.3 FlashAttention\\nTo speed up attention on hardware accelerators such as GPU, [5] proposes an algorithm to reduce the memory\\nreads/writes while maintaining the same output (without approximation).\\n2.3.1 Forward pass'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 2, 'page_label': '3'}, page_content='reads/writes while maintaining the same output (without approximation).\\n2.3.1 Forward pass\\nFlashAttention applies the classical technique of tiling to reduce memory IOs, by (1) loading blocks of\\ninputs from HBM to SRAM, (2) computing attention with respect to that block, and then (3) updating the\\noutput without writing the large intermediate matricesS and P to HBM. As the softmax couples entire rows\\nor blocks of row, online softmax [11, 13] can split the attention computation into blocks, and rescale the\\noutput of each block to finally get the right result (with no approximation). By significantly reducing the\\namount of memory reads/writes,FlashAttention yields 2-4×wall-clock speedup over optimized baseline\\nattention implementations.\\nWe describe the online softmax technique [11] and how it is used in attention [13]. For simplicity, consider\\njust one row block of the attention matrixS, of the form\\n\\x02\\nS(1) S(2)\\x03\\nfor some matricesS(1),S(2) ∈R𝐵𝑟 ×𝐵𝑐 ,'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 2, 'page_label': '3'}, page_content='just one row block of the attention matrixS, of the form\\n\\x02\\nS(1) S(2)\\x03\\nfor some matricesS(1),S(2) ∈R𝐵𝑟 ×𝐵𝑐 ,\\nwhere 𝐵𝑟 and 𝐵𝑐 are the row and column block sizes. We want to compute softmax of this row block and\\nmultiply with the value, of the form\\n\\x14V(1)\\nV(2)\\n\\x15\\nfor some matricesV(1),V(2) ∈R𝐵𝑐 ×𝑑. Standard softmax would\\n2For clarity of exposition, we omit the scaling ofQK⊤ (typically by1/d), and optionally elementwise masking onS and/or\\ndropout applied toP\\n3'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 3, 'page_label': '4'}, page_content='compute:\\n𝑚= max(rowmax(S(1)),rowmax(S(2)))∈ R𝐵𝑟\\nℓ= rowsum(𝑒S(1)−𝑚)+rowsum(𝑒S(2)−𝑚)∈ R𝐵𝑟\\nP =\\n\\x02\\nP(1) P(2)\\x03\\n= diag(ℓ)−1\\nh\\n𝑒S(1)−𝑚 𝑒S(2)−𝑚\\ni\\n∈R𝐵𝑟 ×2𝐵𝑐\\nO =\\n\\x02\\nP(1) P(2)\\x03 \\x14V(1)\\nV(2)\\n\\x15\\n= diag(ℓ)−1𝑒S(1)−𝑚V(1)+𝑒S(2)−𝑚V(2) ∈R𝐵𝑟 ×𝑑.\\nOnline softmax instead computes “local” softmax with respect to each block and rescale to get the right\\noutput at the end:\\n𝑚(1) = rowmax(S(1))∈ R𝐵𝑟\\nℓ(1) = rowsum(𝑒S(1)−𝑚(1)\\n)∈ R𝐵𝑟\\n˜P(1) = diag(ℓ(1))−1𝑒S(1)−𝑚(1)\\n∈R𝐵𝑟 ×𝐵𝑐\\nO(1) = ˜P(1)V(1) = diag(ℓ(1))−1𝑒S(1)−𝑚(1)\\nV(1) ∈R𝐵𝑟 ×𝑑\\n𝑚(2) = max(𝑚(1),rowmax(S(2)))= 𝑚\\nℓ(2) = 𝑒𝑚(1)−𝑚(2)\\nℓ(1)+rowsum(𝑒S(2)−𝑚(2)\\n)= rowsum(𝑒S(1)−𝑚)+rowsum(𝑒S(2)−𝑚)= ℓ\\n˜P(2) = diag(ℓ(2))−1𝑒S(2)−𝑚(2)\\nO(2) = diag(ℓ(1)/ℓ(2))−1O(1)+˜P(2)V(2) = diag(ℓ(2))−1𝑒𝑠(1)−𝑚V(1)+diag(ℓ(2))−1𝑒𝑠(2)−𝑚V(2) = O.\\nWeshowhow FlashAttentionusesonlinesoftmaxtoenabletiling(Fig.1)toreducememoryreads/writes.\\nFigure 1: Diagram of howFlashAttention forward pass is performed, when the keyK is partitioned into'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 3, 'page_label': '4'}, page_content='Figure 1: Diagram of howFlashAttention forward pass is performed, when the keyK is partitioned into\\ntwo blocks and the valueV is also partitioned into two blocks. By computing attention with respect to\\neach block and rescaling the output, we get the right answer at the end, while avoiding expensive memory\\nreads/writes of the intermediate matricesS and P. We simplify the diagram, omitting the step in softmax\\nthat subtracts each element by the row-wise max.\\n4'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 4, 'page_label': '5'}, page_content='2.3.2 Backward pass\\nIn the backward pass, by re-computing the values of the attention matricesS and P once blocks of inputs\\nQ,K,V are already loaded to SRAM,FlashAttention avoids having to store large intermediate values. By\\nnot having to save the large matricesS and P of size𝑁×𝑁, FlashAttention yields 10-20×memory saving\\ndepending on sequence length (memory required in linear in sequence length𝑁 instead of quadratic). The\\nbackward pass also achieves 2-4×wall-clock speedup due to reduce memory reads/writes.\\nThe backward pass applies tiling to the equations in Section 2.2. Though the backward pass is simpler\\nthan the forward pass conceptually (there is no softmax rescaling), the implementation is significantly more\\ninvolved. This is because there are more values to be kept in SRAM to perform 5 matrix multiples in the\\nbackward pass, compared to just 2 matrix multiples in the forward pass.\\n3 FlashAttention-2: Algorithm, Parallelism, and Work Partition-\\ning'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 4, 'page_label': '5'}, page_content='3 FlashAttention-2: Algorithm, Parallelism, and Work Partition-\\ning\\nWe describe theFlashAttention-2 algorithm, which includes several tweaks toFlashAttention to reduce\\nthe number of non-matmul FLOPs. We then describe how to parallelize the computation on different thread\\nblocks to make full use the GPU resources. Finally we describe we partition the work between different warps\\nwithin one thread block to reduce the amount of shared memory access. These improvements lead to 2-3×\\nspeedup as validated in Section 4.\\n3.1 Algorithm\\nWe tweak the algorithm fromFlashAttention to reduce the number of non-matmul FLOPs. This is\\nbecause modern GPUs have specialized compute units (e.g., Tensor Cores on Nvidia GPUs) that makes\\nmatmul much faster. As an example, the A100 GPU has a max theoretical throughput of 312 TFLOPs/s of\\nFP16/BF16 matmul, but only 19.5 TFLOPs/s of non-matmul FP32. Another way to think about this is that'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 4, 'page_label': '5'}, page_content='FP16/BF16 matmul, but only 19.5 TFLOPs/s of non-matmul FP32. Another way to think about this is that\\neach non-matmul FLOP is 16×more expensive than a matmul FLOP. To maintain high throughput (e.g.,\\nmore than 50% of the maximum theoretical TFLOPs/s), we want to spend as much time on matmul FLOPs\\nas possible.\\n3.1.1 Forward pass\\nWe revisit the online softmax trick as shown in Section 2.3 and make two minor tweaks to reduce non-matmul\\nFLOPs:\\n1. We do not have to rescale both terms of the output update bydiag(ℓ(2))−1:\\nO(2) = diag(ℓ(1)/ℓ(2))−1O(1)+diag(ℓ(2))−1𝑒S(2)−𝑚(2)\\nV(2).\\nWe can instead maintain an “un-scaled” version ofO(2) and keep around the statisticsℓ(2):\\n˜O(2) = diag(ℓ(1))−1O(1)+𝑒S(2)−𝑚(2)\\nV(2).\\nOnly at the every end of the loop do we scale the final˜O(last) by diag(ℓ(last))−1 to get the right output.\\n2. We do not have to save both the max𝑚(𝑗) and the sum of exponentialsℓ(𝑗) for the backward pass. We\\nonly need to store the logsumexp𝐿(𝑗) = 𝑚(𝑗)+log(ℓ(𝑗)).\\n5'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 5, 'page_label': '6'}, page_content='In the simple case of 2 blocks in Section 2.3, the online softmax trick now becomes:\\n𝑚(1) = rowmax(S(1))∈ R𝐵𝑟\\nℓ(1) = rowsum(𝑒S(1)−𝑚(1)\\n)∈ R𝐵𝑟\\n˜O(1) = 𝑒S(1)−𝑚(1)\\nV(1) ∈R𝐵𝑟 ×𝑑\\n𝑚(2) = max(𝑚(1),rowmax(S(2)))= 𝑚\\nℓ(2) = 𝑒𝑚(1)−𝑚(2)\\nℓ(1)+rowsum(𝑒S(2)−𝑚(2)\\n)= rowsum(𝑒S(1)−𝑚)+rowsum(𝑒S(2)−𝑚)= ℓ\\n˜P(2) = diag(ℓ(2))−1𝑒S(2)−𝑚(2)\\n˜O(2) = diag(𝑒𝑚(1)−𝑚(2)\\n)−1 ˜O(1)+𝑒S(2)−𝑚(2)\\nV(2) = 𝑒𝑠(1)−𝑚V(1)+𝑒𝑠(2)−𝑚V(2)\\nO(2) = diag(ℓ(2))−1 ˜O(2) = O.\\nWe describe the fullFlashAttention-2 forward pass in Algorithm 1.\\nAlgorithm 1FlashAttention-2 forward pass\\nRequire: Matrices Q,K,V ∈R𝑁×𝑑 in HBM, block sizes𝐵𝑐, 𝐵𝑟.\\n1: Divide Q into 𝑇𝑟 =\\nl\\n𝑁\\n𝐵𝑟\\nm\\nblocks Q1,..., Q𝑇𝑟 of size𝐵𝑟 ×𝑑 each, and divideK,V in to𝑇𝑐 =\\nl\\n𝑁\\n𝐵𝑐\\nm\\nblocks\\nK1,..., K𝑇𝑐 and V1,..., V𝑇𝑐 , of size𝐵𝑐 ×𝑑 each.\\n2: Divide the outputO ∈R𝑁×𝑑 into 𝑇𝑟 blocks O𝑖,..., O𝑇𝑟 of size𝐵𝑟 ×𝑑 each, and divide the logsumexp𝐿\\ninto 𝑇𝑟 blocks 𝐿𝑖,...,𝐿 𝑇𝑟 of size𝐵𝑟 each.\\n3: for 1 ≤𝑖 ≤𝑇𝑟 do\\n4: Load Q𝑖 from HBM to on-chip SRAM.\\n5: On chip, initializeO(0)\\n𝑖 = (0)𝐵𝑟 ×𝑑 ∈R𝐵𝑟 ×𝑑,ℓ(0)'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 5, 'page_label': '6'}, page_content='3: for 1 ≤𝑖 ≤𝑇𝑟 do\\n4: Load Q𝑖 from HBM to on-chip SRAM.\\n5: On chip, initializeO(0)\\n𝑖 = (0)𝐵𝑟 ×𝑑 ∈R𝐵𝑟 ×𝑑,ℓ(0)\\n𝑖 = (0)𝐵𝑟 ∈R𝐵𝑟 ,𝑚(0)\\n𝑖 = (−∞)𝐵𝑟 ∈R𝐵𝑟 .\\n6: for 1 ≤𝑗 ≤𝑇𝑐 do\\n7: Load K𝑗,V𝑗 from HBM to on-chip SRAM.\\n8: On chip, computeS(𝑗)\\n𝑖 = Q𝑖K𝑇\\n𝑗 ∈R𝐵𝑟 ×𝐵𝑐 .\\n9: On chip, compute 𝑚(𝑗)\\n𝑖 = max(𝑚(𝑗−1)\\n𝑖 ,rowmax(S(𝑗)\\n𝑖 )) ∈R𝐵𝑟 , ˜P(𝑗)\\n𝑖 = exp(S(𝑗)\\n𝑖 −𝑚(𝑗)\\n𝑖 ) ∈R𝐵𝑟 ×𝐵𝑐\\n(pointwise), ℓ(𝑗)\\n𝑖 = 𝑒𝑚𝑗−1\\n𝑖 −𝑚(𝑗)\\n𝑖 ℓ(𝑗−1)\\n𝑖 +rowsum(˜P(𝑗)\\n𝑖 )∈ R𝐵𝑟 .\\n10: On chip, computeO(𝑗)\\n𝑖 = diag(𝑒𝑚(𝑗−1)\\n𝑖 −𝑚(𝑗)\\n𝑖 )−1O(𝑗−1)\\n𝑖 +˜P(𝑗)\\n𝑖 V𝑗.\\n11: end for\\n12: On chip, computeO𝑖 = diag(ℓ(𝑇𝑐 )\\n𝑖 )−1O(𝑇𝑐 )\\n𝑖 .\\n13: On chip, compute𝐿𝑖 = 𝑚(𝑇𝑐 )\\n𝑖 +log(ℓ(𝑇𝑐 )\\n𝑖 ).\\n14: Write O𝑖 to HBM as the𝑖-th block ofO.\\n15: Write 𝐿𝑖 to HBM as the𝑖-th block of𝐿.\\n16: end for\\n17: Return the outputO and the logsumexp𝐿.\\nCausal masking. One common use case of attention is in auto-regressive language modeling, where we\\nneed to apply a causal mask to the attention matrixS (i.e., any entryS𝑖𝑗 with 𝑗 >𝑖 is set to−∞).'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 5, 'page_label': '6'}, page_content='need to apply a causal mask to the attention matrixS (i.e., any entryS𝑖𝑗 with 𝑗 >𝑖 is set to−∞).\\n1. As FlashAttention and FlashAttention-2 already operate by blocks, for any blocks where all\\nthe column indices are more than the row indices (approximately half of the blocks for large sequence\\nlength), we can skip the computation of that block. This leads to around 1.7-1.8×speedup compared\\nto attention without the causal mask.\\n2. We do not need to apply the causal mask for blocks whose row indices are guaranteed to be strictly less\\nthan the column indices. This means that for each row, we only need apply causal mask to 1 block\\n(assuming square block).\\n6'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 6, 'page_label': '7'}, page_content='Correctness, runtime, and memory requirement.As withFlashAttention, Algorithm 1 returns\\nthe correct outputO = softmax(QK⊤)V (with no approximation), using𝑂(𝑁2𝑑)FLOPs and requires𝑂(𝑁)\\nadditional memory beyond inputs and output (to store the logsumexp𝐿). The proof is almost the same as\\nthe proof of Dao et al. [5, Theorem 1], so we omit it here.\\n3.1.2 Backward pass\\nThe backward pass ofFlashAttention-2 is almost the same as that ofFlashAttention. We make a\\nminor tweak to only use the row-wise logsumexp𝐿 instead of both the row-wise max and row-wise sum of\\nexponentials in the softmax. We include the backward pass description in Algorithm 2 for completeness.\\nAlgorithm 2FlashAttention-2 Backward Pass\\nRequire: Matrices Q,K,V,O,dO ∈R𝑁×𝑑 in HBM, vector𝐿 ∈R𝑁 in HBM, block sizes𝐵𝑐, 𝐵𝑟.\\n1: Divide Q into 𝑇𝑟 =\\nl\\n𝑁\\n𝐵𝑟\\nm\\nblocks Q1,..., Q𝑇𝑟 of size𝐵𝑟 ×𝑑 each, and divideK,V in to𝑇𝑐 =\\nl\\n𝑁\\n𝐵𝑐\\nm\\nblocks\\nK1,..., K𝑇𝑐 and V1,..., V𝑇𝑐 , of size𝐵𝑐 ×𝑑 each.'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 6, 'page_label': '7'}, page_content='l\\n𝑁\\n𝐵𝑐\\nm\\nblocks\\nK1,..., K𝑇𝑐 and V1,..., V𝑇𝑐 , of size𝐵𝑐 ×𝑑 each.\\n2: Divide O into 𝑇𝑟 blocks O𝑖,..., O𝑇𝑟 of size𝐵𝑟 ×𝑑 each, dividedO into 𝑇𝑟 blocks dO𝑖,..., dO𝑇𝑟 of size\\n𝐵𝑟 ×𝑑 each, and divide𝐿 into 𝑇𝑟 blocks 𝐿𝑖,...,𝐿 𝑇𝑟 of size𝐵𝑟 each.\\n3: Initialize dQ = (0)𝑁×𝑑 in HBM and divide it into𝑇𝑟 blocks dQ1,..., dQ𝑇𝑟 of size 𝐵𝑟 ×𝑑 each. Divide\\ndK,dV ∈R𝑁×𝑑 in to𝑇𝑐 blocks dK1,..., dK𝑇𝑐 and dV1,..., dV𝑇𝑐 , of size𝐵𝑐 ×𝑑 each.\\n4: Compute 𝐷 = rowsum(dO ◦O)∈ R𝑑 (pointwise multiply), write𝐷 to HBM and divide it into𝑇𝑟 blocks\\n𝐷1,...,𝐷 𝑇𝑟 of size𝐵𝑟 each.\\n5: for 1 ≤𝑗 ≤𝑇𝑐 do\\n6: Load K𝑗,V𝑗 from HBM to on-chip SRAM.\\n7: Initialize dK𝑗 = (0)𝐵𝑐 ×𝑑,dV𝑗 = (0)𝐵𝑐 ×𝑑 on SRAM.\\n8: for 1 ≤𝑖 ≤𝑇𝑟 do\\n9: Load Q𝑖,O𝑖,dO𝑖,dQ𝑖,𝐿𝑖,𝐷𝑖 from HBM to on-chip SRAM.\\n10: On chip, computeS(𝑗)\\n𝑖 = Q𝑖K𝑇\\n𝑗 ∈R𝐵𝑟 ×𝐵𝑐 .\\n11: On chip, computeP(𝑗)\\n𝑖 = exp(S𝑖𝑗 −𝐿𝑖)∈ R𝐵𝑟 ×𝐵𝑐 .\\n12: On chip, computedV𝑗 ←dV𝑗 +(P(𝑗)\\n𝑖 )⊤dO𝑖 ∈R𝐵𝑐 ×𝑑.\\n13: On chip, computedP(𝑗)\\n𝑖 = dO𝑖V⊤\\n𝑗 ∈R𝐵𝑟 ×𝐵𝑐 .\\n14: On chip, computedS(𝑗)\\n𝑖 = P(𝑗)\\n𝑖 ◦(dP(𝑗)\\n𝑖 −𝐷𝑖)∈ R𝐵𝑟 ×𝐵𝑐 .'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 6, 'page_label': '7'}, page_content='13: On chip, computedP(𝑗)\\n𝑖 = dO𝑖V⊤\\n𝑗 ∈R𝐵𝑟 ×𝐵𝑐 .\\n14: On chip, computedS(𝑗)\\n𝑖 = P(𝑗)\\n𝑖 ◦(dP(𝑗)\\n𝑖 −𝐷𝑖)∈ R𝐵𝑟 ×𝐵𝑐 .\\n15: Load dQ𝑖 from HBM to SRAM, then on chip, updatedQ𝑖 ←dQ𝑖 +dS(𝑗)\\n𝑖 K𝑗 ∈R𝐵𝑟 ×𝑑, and write back\\nto HBM.\\n16: On chip, computedK𝑗 ←dK𝑗 +dS(𝑗)\\n𝑖\\n⊤\\nQ𝑖 ∈R𝐵𝑐 ×𝑑.\\n17: end for\\n18: Write dK𝑗,dV𝑗 to HBM.\\n19: end for\\n20: Return dQ,dK,dV.\\nMulti-query attention and grouped-query attention.Multi-query attention (MQA) [15] and grouped-\\nquery attention (GQA) [1] are variants of attention where multiple heads of query attend to the same head of\\nkey and value, in order to reduce the size of KV cache during inference. Instead of having to duplicate the\\nkey and value heads for the computation, we implicitly manipulate the indices into the head to perform the\\nsame computation. In the backward pass, we need to sum the gradientsdK and dV across different heads\\nthat were implicitly duplicated.\\n3.2 Parallelism'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 6, 'page_label': '7'}, page_content='that were implicitly duplicated.\\n3.2 Parallelism\\nThe first version ofFlashAttention parallelizes over batch size and number of heads. We use 1 thread\\nblock to process one attention head, and there are overallbatch size·number of headsthread blocks. Each\\nthread block is scheduled to run on a streaming multiprocessor (SM), and there are 108 of these SMs on\\n7'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 7, 'page_label': '8'}, page_content='an A100 GPU for example. This scheduling is efficient when this number is large (say≥80), since we can\\neffectively use almost all of the compute resources on the GPU.\\nIn the case of long sequences (which usually means small batch sizes or small number of heads), to make\\nbetter use of the multiprocessors on the GPU, we now additionally parallelize over the sequence length\\ndimension. This results in significant speedup for this regime.\\nForward pass. We see that the outer loop (over sequence length) is embarrassingly parallel, and we\\nschedule them on different thread blocks that do not need to communicate with each other. We also parallelize\\nover the batch dimension and number of heads dimension, as done inFlashAttention. The increased\\nparallelism over sequence length helps improve occupancy (fraction of GPU resources being used) when the\\nbatch size and number of heads are small, leading to speedup in this case.'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 7, 'page_label': '8'}, page_content='batch size and number of heads are small, leading to speedup in this case.\\nThese ideas of swapping the order of the loop (outer loop over row blocks and inner loop over column\\nblocks, instead of the other way round in the originalFlashAttention paper), as well as parallelizing\\nover the sequence length dimension were first suggested and implemented by Phil Tillet in the Triton [17]\\nimplementation.3\\nBackward pass. Notice that the only shared computation between different column blocks is in updatedQ\\nin Algorithm 2, where we need to loaddQ𝑖 from HBM to SRAM, then on chip, updatedQ𝑖 ←dQ𝑖 +dS(𝑗)\\n𝑖 K𝑗,\\nand write back to HBM. We thus parallelize over the sequence length dimension as well, and schedule 1\\nthread block for each column block of the backward pass. We use atomic adds to communicate between\\ndifferent thread blocks to updatedQ.\\nWe describe the parallelization scheme in Fig. 2.'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 7, 'page_label': '8'}, page_content='different thread blocks to updatedQ.\\nWe describe the parallelization scheme in Fig. 2.\\nFigure 2: In the forward pass (left), we parallelize the workers (thread blocks) where each worker takes care\\nof a block of rows of the attention matrix. In the backward pass (right), each worker takes care of a block of\\ncolumns of the attention matrix.\\n3https://github.com/openai/triton/blob/main/python/tutorials/06-fused-attention.py\\n8'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 8, 'page_label': '9'}, page_content='3.3 Work Partitioning Between Warps\\nAs Section 3.2 describe how we schedule thread blocks, even within each thread block, we also have to decide\\nhow to partition the work between different warps. We typically use 4 or 8 warps per thread block, and the\\npartitioning is described in Fig. 3.\\nForward pass. For each block,FlashAttention splits K and V across 4 warps while keepingQ accessible\\nby all warps. Each warp multiplies to get a slice ofQK⊤, then they need to multiply with a slice ofV and\\ncommunicate to add up the result. This is referred to as the “split-K” scheme. However, this is inefficient\\nsince all warps need to write their intermediate results out to shared memory, synchronize, then add up the\\nintermediate results. These shared memory reads/writes slow down the forward pass inFlashAttention.\\nIn FlashAttention-2, we instead splitQ across 4 warps while keepingK and V accessible by all warps.'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 8, 'page_label': '9'}, page_content='In FlashAttention-2, we instead splitQ across 4 warps while keepingK and V accessible by all warps.\\nAfter each warp performs matrix multiply to get a slice ofQK⊤, they just need to multiply with their shared\\nslice ofV to get their corresponding slice of the output. There is no need for communication between warps.\\nThe reduction in shared memory reads/writes yields speedup (Section 4).\\n(a) FlashAttention\\n (b) FlashAttention-2\\nFigure 3: Work partitioning between different warps in the forward pass\\nBackward pass. Similarly for the backward pass, we choose to partition the warps to avoid the “split-K”\\nscheme. However, it still requires some synchronization due to the more complicated dependency between all\\nthe different inputs and gradientsQ,K,V,O,dO,dQ,dK,dV. Nevertheless, avoiding “split-K” reduces shared\\nmemory reads/writes and again yields speedup (Section 4).\\nTuning block sizes Increasing block sizes generally reduces shared memory loads/stores, but increases'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 8, 'page_label': '9'}, page_content='Tuning block sizes Increasing block sizes generally reduces shared memory loads/stores, but increases\\nthe number of registers required and the total amount of shared memory. Past a certain block size, register\\nspilling causes significant slowdown, or the amount of shared memory required is larger than what the GPU\\nhas available, and the kernel cannot run at all. Typically we choose blocks of size{64,128}×{ 64,128},\\ndepending on the head dimension𝑑 and the device shared memory size.\\nWe manually tune for each head dimensions since there are essentially only 4 choices for block sizes, but\\nthis could benefit from auto-tuning to avoid this manual labor. We leave this to future work.\\n4 Empirical Validation\\nWe evaluate the impact of usingFlashAttention-2 to train Transformer models.\\n• Benchmarking attention.We measure the runtime ofFlashAttention-2 across different sequence\\nlengthsandcompareittoastandardimplementationinPyTorch, FlashAttention, andFlashAttention'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 8, 'page_label': '9'}, page_content='lengthsandcompareittoastandardimplementationinPyTorch, FlashAttention, andFlashAttention\\nin Triton. We confirm that FlashAttention-2 is 1.7-3.0×faster than FlashAttention, 1.3-2.5×\\nfaster than FlashAttention in Triton, and 3-10×faster than a standard attention implementation.\\n9'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 9, 'page_label': '10'}, page_content='FlashAttention-2 reaches up to 230 TFLOPs/s, 73% of the theoretical maximum TFLOPs/s on A100\\nGPUs.\\n• End-to-end training speedWhen used end-to-end to train GPT-style models of size 1.3B and 2.7B on\\nsequence lengths either 2k or 8k,FlashAttention-2 yields up to 1.3×speedup compared toFlashAt-\\ntention and 2.8×speedup compared to a baseline withoutFlashAttention. FlashAttention-2\\nreaches up to 225 TFLOPs/s (72% model FLOPs utilization) per A100 GPU.\\n4.1 Benchmarking Attention\\nWe measure the runtime of different attention methods on an A100 80GB SXM4 GPU for different settings\\n(without / with causal mask, head dimension 64 or 128). We report the results in Fig. 4, Fig. 5 and Fig. 6,\\nshowing thatFlashAttention-2 is around 2×faster thanFlashAttention and FlashAttention in\\nxformers (the “cutlass” implementation).FlashAttention-2 is around 1.3-1.5×faster thanFlashAtten-\\ntion in Triton in the forward pass and around 2×faster in the backward pass. Compared to a standard'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 9, 'page_label': '10'}, page_content='tion in Triton in the forward pass and around 2×faster in the backward pass. Compared to a standard\\nattention implementation in PyTorch,FlashAttention-2 can be up to 10×faster.\\nBenchmark setting: we vary the sequence length from 512, 1k, ..., 16k, and set batch size so that the total\\nnumber of tokens is 16k. We set hidden dimension to 2048, and head dimension to be either 64 or 128 (i.e.,\\n32 heads or 16 heads). To calculate the FLOPs of the forward pass, we use:\\n4 ·seqlen2 ·head dimension·number of heads.\\nWith causal mask, we divide this number by 2 to account for the fact that approximately only half of the\\nentries are calculated. To get the FLOPs of the backward pass, we multiply the forward pass FLOPs by 2.5\\n(since there are 2 matmuls in the forward pass and 5 matmuls in the backward pass, due to recomputation).\\n512 1k 2k 4k 8k 16k\\nSequence length\\n50\\n100\\n150\\n200Speed (TFLOPs/s)\\n36 40 43 45 46\\nOOM\\n91 92\\n104 108 110 110\\n68 73 76 77 75 75\\n90\\n102 98 98 100 100\\n132\\n153\\n162\\n171 175 176'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 9, 'page_label': '10'}, page_content='36 40 43 45 46\\nOOM\\n91 92\\n104 108 110 110\\n68 73 76 77 75 75\\n90\\n102 98 98 100 100\\n132\\n153\\n162\\n171 175 176\\nAttention forward + backward speed (A100 80GB SXM4)\\nPytorch\\nFlashAttention\\nxformers\\nFlashAttention Triton\\nFlashAttention-2\\n(a) Without causal mask, head dimension 64\\n512 1k 2k 4k 8k 16k\\nSequence length\\n50\\n100\\n150\\n200Speed (TFLOPs/s)53\\n67\\n76 79 86\\nOOM\\n76 72 76 80 82 8383\\n91 95 96 97 98\\n78 85 90 93 95 95\\n151\\n173\\n187\\n196 201 203\\nAttention forward + backward speed (A100 80GB SXM4)\\nPytorch\\nFlashAttention\\nxformers\\nFlashAttention Triton\\nFlashAttention-2 (b) Without causal mask, head dimension 128\\n512 1k 2k 4k 8k 16k\\nSequence length\\n50\\n100\\n150\\n200Speed (TFLOPs/s)\\n15 16 17 18 18\\nOOM\\n58\\n70\\n77\\n87 92 97\\n51\\n60 66 68 69 67\\n59\\n75 79 76 79 80\\n88\\n119\\n140\\n156\\n165 171\\nAttention forward + backward speed (A100 80GB SXM4)\\nPytorch\\nFlashAttention\\nxformers\\nFlashAttention Triton\\nFlashAttention-2\\n(c) With causal mask, head dimension 64\\n512 1k 2k 4k 8k 16k\\nSequence length\\n50\\n100\\n150\\n200Speed (TFLOPs/s)'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 9, 'page_label': '10'}, page_content='(c) With causal mask, head dimension 64\\n512 1k 2k 4k 8k 16k\\nSequence length\\n50\\n100\\n150\\n200Speed (TFLOPs/s)\\n23 28 32 32 34\\nOOM\\n55 62\\n69 76 80 83\\n58\\n72\\n82 87 91 92\\n50\\n61\\n68 74 78 80\\n99\\n133\\n155\\n173\\n182 189\\nAttention forward + backward speed (A100 80GB SXM4)\\nPytorch\\nFlashAttention\\nxformers\\nFlashAttention Triton\\nFlashAttention-2 (d) With causal mask, head dimension 128\\nFigure 4: Attention forward + backward speed on A100 GPU\\n10'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 10, 'page_label': '11'}, page_content='512 1k 2k 4k 8k 16k\\nSequence length\\n50\\n100\\n150\\n200Speed (TFLOPs/s)\\n29 34 35 37 37\\nOOM\\n91 96 99 104 104 104\\n89 94 97 99 98 98\\n128\\n141\\n149 152 152 155\\n178\\n191 193 192 192 192\\nAttention forward speed (A100 80GB SXM4)\\nPytorch\\nFlashAttention\\nxformers\\nFlashAttention Triton\\nFlashAttention-2\\n(a) Without causal mask, head dimension 64\\n512 1k 2k 4k 8k 16k\\nSequence length\\n50\\n100\\n150\\n200Speed (TFLOPs/s)42\\n56 60 63 67\\nOOM\\n69 66 71 71 72 73\\n107\\n115 120 122 122 122127\\n140\\n152 157 160 163\\n209\\n224 227 222 224 223\\nAttention forward speed (A100 80GB SXM4)\\nPytorch\\nFlashAttention\\nxformers\\nFlashAttention Triton\\nFlashAttention-2 (b) Without causal mask, head dimension 128\\n512 1k 2k 4k 8k 16k\\nSequence length\\n50\\n100\\n150\\n200Speed (TFLOPs/s)\\n10 10 10 10 10\\nOOM\\n56\\n70\\n81\\n89 91 94\\n71\\n82 89 92 94 95\\n78\\n99\\n112\\n131 137 143\\n115\\n146\\n167\\n177 181 183\\nAttention forward speed (A100 80GB SXM4)\\nPytorch\\nFlashAttention\\nxformers\\nFlashAttention Triton\\nFlashAttention-2\\n(c) With causal mask, head dimension 64\\n512 1k 2k 4k 8k 16k'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 10, 'page_label': '11'}, page_content='xformers\\nFlashAttention Triton\\nFlashAttention-2\\n(c) With causal mask, head dimension 64\\n512 1k 2k 4k 8k 16k\\nSequence length\\n50\\n100\\n150\\n200Speed (TFLOPs/s)\\n15 18 19 19 19\\nOOM\\n49\\n59 65 68 70 71\\n79\\n95\\n107 112 115 117\\n89\\n108\\n126\\n133\\n141\\n148\\n132\\n168\\n187\\n198 200 197\\nAttention forward speed (A100 80GB SXM4)\\nPytorch\\nFlashAttention\\nxformers\\nFlashAttention Triton\\nFlashAttention-2 (d) With causal mask, head dimension 128\\nFigure 5: Attention forward speed on A100 GPU\\nJust running the same implementation on H100 GPUs (using no special instructions to make use of new\\nfeatures such as TMA and 4th-gen Tensor Cores), we obtain up to 335 TFLOPs/s (Fig. 7). We expect that by\\nusing new instructions, we can obtain another 1.5x-2x speedup on H100 GPUs. We leave that to future work.\\n4.2 End-to-end Performance\\nWe measure the training throughput of GPT-style models with either 1.3B or 2.7B parameters, on 8×A100\\n80GB SXM. As shown in Table 1,FlashAttention-2 yields 2.8×speedup compared to a baseline without'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 10, 'page_label': '11'}, page_content='80GB SXM. As shown in Table 1,FlashAttention-2 yields 2.8×speedup compared to a baseline without\\nFlashAttention and 1.3×speedup compared toFlashAttention-2, reaching up to 225 TFLOPs/s per A100\\nGPU.\\nNote that we calculate the FLOPs by the formula, following Megatron-LM [16] (and many other papers\\nand libraries):\\n6 ·seqlen·number of params+12 ·number of layers·hidden dim·seqlen2.\\nThe first term accounts for the FLOPs due to weight–input multiplication, and the second term accounts for\\nthe FLOPs due to attention. However, one can argue that the second term should be halved, as with causal\\nmask we only need to compute approximately half the number of elements in attention. We choose to follow\\nthe formula from the literature (without dividing the attention FLOPs by 2) for consistency.\\n5 Discussion and Future Directions\\nFlashAttention-2 is 2×faster thanFlashAttention, which means that we can train models with 16k'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 10, 'page_label': '11'}, page_content='FlashAttention-2 is 2×faster thanFlashAttention, which means that we can train models with 16k\\nlonger context for the same price as previously training a 8k context model. We are excited about how this can\\n11'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 11, 'page_label': '12'}, page_content='512 1k 2k 4k 8k 16k\\nSequence length\\n50\\n100\\n150\\n200Speed (TFLOPs/s)\\n39 43 48 49 51\\nOOM\\n91 90\\n106 109 112 113\\n62 67 70 70 69 68\\n81\\n92 87 86 87 88\\n120\\n141\\n152\\n163 169 170\\nAttention backward speed (A100 80GB SXM4)\\nPytorch\\nFlashAttention\\nxformers\\nFlashAttention Triton\\nFlashAttention-2\\n(a) Without causal mask, head dimension 64\\n512 1k 2k 4k 8k 16k\\nSequence length\\n50\\n100\\n150\\n200Speed (TFLOPs/s)\\n59\\n73\\n86 88\\n97\\nOOM\\n78 75 79 84 86 88\\n76\\n84 88 89 90 91\\n68 74 77 80 82 81\\n136\\n159\\n175\\n187 193 196\\nAttention backward speed (A100 80GB SXM4)\\nPytorch\\nFlashAttention\\nxformers\\nFlashAttention Triton\\nFlashAttention-2 (b) Without causal mask, head dimension 128\\n512 1k 2k 4k 8k 16k\\nSequence length\\n50\\n100\\n150\\n200Speed (TFLOPs/s)\\n19 21 24 25 26\\nOOM\\n58\\n70 76\\n85\\n93 98\\n46\\n54 60 62 62 6053\\n68 71 65 67 68\\n81\\n111\\n131\\n149\\n160 166\\nAttention backward speed (A100 80GB SXM4)\\nPytorch\\nFlashAttention\\nxformers\\nFlashAttention Triton\\nFlashAttention-2\\n(c) With causal mask, head dimension 64\\n512 1k 2k 4k 8k 16k\\nSequence length\\n50'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 11, 'page_label': '12'}, page_content='FlashAttention Triton\\nFlashAttention-2\\n(c) With causal mask, head dimension 64\\n512 1k 2k 4k 8k 16k\\nSequence length\\n50\\n100\\n150\\n200Speed (TFLOPs/s)\\n30\\n37 43 45 49\\nOOM\\n59 63\\n71\\n80 86 89\\n53\\n65\\n75 80 84 84\\n43\\n52 58 63 66 67\\n90\\n122\\n145\\n165\\n176\\n186\\nAttention backward speed (A100 80GB SXM4)\\nPytorch\\nFlashAttention\\nxformers\\nFlashAttention Triton\\nFlashAttention-2 (d) With causal mask, head dimension 128\\nFigure 6: Attention backward speed on A100 GPU\\nTable 1: Training speed (TFLOPs/s/GPU) of GPT-style models on 8×A100 GPUs. FlashAttention-2\\nreaches up to 225 TFLOPs/s (72% model FLOPs utilization). We compare against a baseline running without\\nFlashAttention.\\nModel Without FlashAttention FlashAttention FlashAttention-2\\nGPT3-1.3B 2k context 142 TFLOPs/s 189 TFLOPs/s 196 TFLOPs/s\\nGPT3-1.3B 8k context 72 TFLOPS/s 170 TFLOPs/s 220 TFLOPs/s\\nGPT3-2.7B 2k context 149 TFLOPs/s 189 TFLOPs/s 205 TFLOPs/s\\nGPT3-2.7B 8k context 80 TFLOPs/s 175 TFLOPs/s 225 TFLOPs/s'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 11, 'page_label': '12'}, page_content='GPT3-2.7B 2k context 149 TFLOPs/s 189 TFLOPs/s 205 TFLOPs/s\\nGPT3-2.7B 8k context 80 TFLOPs/s 175 TFLOPs/s 225 TFLOPs/s\\nbe used to understand long books and reports, high resolution images, audio and video.FlashAttention-2\\nwill also speed up training, finetuning, and inference of existing models.\\nIn the near future, we plan to collaborate with researchers and engineers to make FlashAttention widely\\napplicable in different kinds of devices (e.g., H100 GPUs, AMD GPUs), as well as new data types such as\\nFP8. As an immediate next step, we plan to optimize FlashAttention-2 for H100 GPUs to use new hardware\\nfeatures (TMA, 4th-gen Tensor Cores, fp8). Combining the low-level optimizations in FlashAttention-2 with\\nhigh-level algorithmic changes (e.g., local, dilated, block-sparse attention) could allow us to train AI models\\nwith much longer context. We are also excited to work with compiler researchers to make these optimization\\ntechniques easily programmable.\\n12'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 12, 'page_label': '13'}, page_content='512 1k 2k 4k 8k 16k\\nSequence length\\n100\\n200\\n300Speed (TFLOPs/s)\\n62 72 81 86 87\\nOOM\\n157 159 161 161 166 168\\n215\\n254\\n274\\n288 294 296\\nAttention forward + backward speed (H100 80GB SXM5)\\nPytorch\\nFlashAttention\\nFlashAttention-2\\n(a) Without causal mask, head dimension 64\\n512 1k 2k 4k 8k 16k\\nSequence length\\n100\\n200\\n300Speed (TFLOPs/s)\\n93\\n120\\n145\\n160 167\\nOOM\\n127 127 128 131 137 139\\n248\\n294\\n320 326 335 338\\nAttention forward + backward speed (H100 80GB SXM5)\\nPytorch\\nFlashAttention\\nFlashAttention-2 (b) Without causal mask, head dimension 128\\n512 1k 2k 4k 8k 16k\\nSequence length\\n100\\n200\\n300Speed (TFLOPs/s)\\n26 29 31 32 32\\nOOM\\n104\\n123\\n136 138 149 156\\n141\\n192\\n232\\n257\\n273 284\\nAttention forward + backward speed (H100 80GB SXM5)\\nPytorch\\nFlashAttention\\nFlashAttention-2\\n(c) With causal mask, head dimension 64\\n512 1k 2k 4k 8k 16k\\nSequence length\\n100\\n200\\n300Speed (TFLOPs/s)\\n40 50 57 61 63\\nOOM\\n98 109 108\\n126 135 137\\n163\\n221\\n265\\n294\\n308\\n328\\nAttention forward + backward speed (H100 80GB SXM5)\\nPytorch'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 12, 'page_label': '13'}, page_content='OOM\\n98 109 108\\n126 135 137\\n163\\n221\\n265\\n294\\n308\\n328\\nAttention forward + backward speed (H100 80GB SXM5)\\nPytorch\\nFlashAttention\\nFlashAttention-2 (d) With causal mask, head dimension 128\\nFigure 7: Attention forward + backward speed on H100 GPU\\nAcknowledgments\\nWethankPhilTilletandDanielHaziza, whohaveimplementedversionsof FlashAttentioninTriton[ 17]and\\nthexformers library[10]. FlashAttention-2wasmotivatedbyexchangeofideasbetweendifferentwaysthat\\nattention could be implemented. We are grateful to the Nvidia CUTLASS team (especially Vijay Thakkar, Cris\\nCecka, Haicheng Wu, and Andrew Kerr) for their CUTLASS library, in particular the CUTLASS 3.x release,\\nwhich provides clean abstractions and powerful building blocks for the implementation ofFlashAttention-2.\\nWe thank Driss Guessous for integratingFlashAttention to PyTorch.FlashAttention-2 has benefited\\nfrom helpful discussions with Phil Wang, Markus Rabe, James Bradbury, Young-Jun Ko, Julien Launay,'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 12, 'page_label': '13'}, page_content='from helpful discussions with Phil Wang, Markus Rabe, James Bradbury, Young-Jun Ko, Julien Launay,\\nDaniel Hesslow, Michaël Benesty, Horace He, Ashish Vaswani, and Erich Elsen. Thanks for Stanford CRFM\\nand Stanford NLP for the compute support. We thank Dan Fu and Christopher Ré for their collaboration,\\nconstructive feedback, and constant encouragement on this line of work of designing hardware-efficient\\nalgorithms. We thank Albert Gu and Beidi Chen for their helpful suggestions on early drafts of this technical\\nreport.\\nReferences\\n[1] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit\\nSanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints.arXiv\\npreprint arXiv:2305.13245, 2023.\\n[2] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.arXiv\\npreprint arXiv:2004.05150, 2020.\\n13'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 13, 'page_label': '14'}, page_content='[3] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher Ré. Scatterbrain: Unifying\\nsparse and low-rank attention. InAdvances in Neural Information Processing Systems (NeurIPS), 2021.\\n[4] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane,\\nTamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking\\nattention with performers. InInternational Conference on Learning Representations (ICLR), 2020.\\n[5] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention: Fast and\\nmemory-efficient exact attention with IO-awareness. InAdvances in Neural Information Processing\\nSystems, 2022.\\n[6] Zhe Jia and Peter Van Sandt. Dissecting the Ampere GPU architecture via microbenchmarking. GPU\\nTechnology Conference, 2021.\\n[7] Zhe Jia, Marco Maggioni, Benjamin Staiger, and Daniele P Scarpazza. Dissecting the nvidia Volta GPU'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 13, 'page_label': '14'}, page_content='[7] Zhe Jia, Marco Maggioni, Benjamin Staiger, and Daniele P Scarpazza. Dissecting the nvidia Volta GPU\\narchitecture via microbenchmarking.arXiv preprint arXiv:1804.06826, 2018.\\n[8] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are RNNs:\\nFast autoregressive transformers with linear attention. InInternational Conference on Machine Learning,\\npages 5156–5165. PMLR, 2020.\\n[9] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. InThe\\nInternational Conference on Machine Learning (ICML), 2020.\\n[10] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren,\\nMin Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel Haziza. xformers: A modular\\nand hackable transformer modelling library.https://github.com/facebookresearch/xformers, 2022.\\n[11] Maxim Milakov and Natalia Gimelshein. Online normalizer calculation for softmax.arXiv preprint\\narXiv:1805.02867, 2018.'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 13, 'page_label': '14'}, page_content='arXiv:1805.02867, 2018.\\n[12] OpenAI. Gpt-4 technical report.ArXiv, abs/2303.08774, 2023.\\n[13] Markus N Rabe and Charles Staats. Self-attention does not need 𝑂(𝑛2)memory. arXiv preprint\\narXiv:2112.05682, 2021.\\n[14] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse\\nattention with routing transformers.Transactions of the Association for Computational Linguistics, 9:\\n53–68, 2021.\\n[15] Noam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint\\narXiv:1911.02150, 2019.\\n[16] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro.\\nMegatron-LM: Training multi-billion parameter language models using model parallelism.arXiv preprint\\narXiv:1909.08053, 2019.\\n[17] Philippe Tillet, Hsiang-Tsung Kung, and David Cox. Triton: an intermediate language and compiler for\\ntiled neural network computations. InProceedings of the 3rd ACM SIGPLAN International Workshop'),\n"," Document(metadata={'source': '/content/data/2307.08691v1.pdf', 'page': 13, 'page_label': '14'}, page_content='tiled neural network computations. InProceedings of the 3rd ACM SIGPLAN International Workshop\\non Machine Learning and Programming Languages, pages 10–19, 2019.\\n[18] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\\nKaiser, and Illia Polosukhin. Attention is all you need.Advances in neural information processing\\nsystems, 30, 2017.\\n[19] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with\\nlinear complexity.arXiv preprint arXiv:2006.04768, 2020.\\n[20] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago\\nOntanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer\\nsequences. Advances in Neural Information Processing Systems, 33, 2020.\\n14')]"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["from langchain.embeddings import FakeEmbeddings\n","\n","embeddings = FakeEmbeddings(size=100)"],"metadata":{"id":"ID_DY6tT3JGW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_community.vectorstores import FAISS\n","vectorestoredb = FAISS.from_documents(docs,embeddings)"],"metadata":{"id":"1_RGces03lvC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vectorestoredb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nn_jmlD05if7","executionInfo":{"status":"ok","timestamp":1738856093807,"user_tz":-330,"elapsed":570,"user":{"displayName":"Brue Pl","userId":"14478803105777518815"}},"outputId":"d4d69233-cb1d-4d97-c21c-be2430febce4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<langchain_community.vectorstores.faiss.FAISS at 0x7b6ab1925990>"]},"metadata":{},"execution_count":50}]},{"cell_type":"code","source":["query=\"what is forward pass?\"\n","result=vectorestoredb.similarity_search(query)\n","result[0].page_content"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":122},"id":"MKJ0haBL4L7b","executionInfo":{"status":"ok","timestamp":1738856194211,"user_tz":-330,"elapsed":1002,"user":{"displayName":"Brue Pl","userId":"14478803105777518815"}},"outputId":"a883c63e-b9f8-4f30-e844-1ac316454ad8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'3. Even within one block of attention computation, we partition the work between different warps of a\\nthread block to reduce communication and shared memory reads/writes.\\nIn Section 4, we empirically validate thatFlashAttention-2 yields significant speedup compared to\\neven FlashAttention. Benchmarks on different settings (with or without causal mask, different head\\ndimensions) show thatFlashAttention-2 achieves around 2×speedup overFlashAttention, reaching\\nup to 73% of the theoretical max throughput in the forward pass, and up to 63% of the theoretical max\\nthroughput in the backward pass. When used end-to-end to train GPT-style models, we reach training speed\\nof up to 225 TFLOPs/s per A100 GPU.\\n2 Background\\nWe provide some background on the performance characteristics and execution model of GPUs. We also\\ndescribe the standard implementation of attention, as well asFlashAttention.\\n2.1 Hardware characteristics'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":52}]},{"cell_type":"code","source":["from langchain.chains import RetrievalQA\n","retriever = vectorestoredb.as_retriever()\n","qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)"],"metadata":{"id":"GnaZy9_K52yu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.prompts import PromptTemplate\n","\n","\n","prompt_template = \"\"\"\n","Use the following pieces of context to answer the question at the end.\n","If you don't know the answer, just say \"I don't know\", don't try to make up an answer.\n","\n","Context: {context}\n","\n","Question: {question}\n","Answer:\n","\"\"\"\n","PROMPT = PromptTemplate(\n","    template=prompt_template, input_variables=[\"context\", \"question\"]\n",")\n"],"metadata":{"id":"PtzCzQ-t78AK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","query = \"What is the main topic discussed in these documents?\" # Example query\n","result = qa({\"query\": query})\n","\n","print(\"Question:\", query)\n","print(\"Answer:\", result[\"result\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2QrjR8rr8OaS","executionInfo":{"status":"ok","timestamp":1738856838466,"user_tz":-330,"elapsed":3074,"user":{"displayName":"Brue Pl","userId":"14478803105777518815"}},"outputId":"b46ea3e6-0b0d-4e11-d587-d41cd912720c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-56-4a2c22a8af1a>:2: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n","  result = qa({\"query\": query})\n"]},{"output_type":"stream","name":"stdout","text":["Question: What is the main topic discussed in these documents?\n","Answer: The documents mainly discuss optimizations and parallelization techniques for attention mechanisms, particularly focusing on:\n","\n","*   **Efficient computation of attention matrices:** This includes skipping computations for certain blocks based on causal masking and block indices to improve speed.\n","*   **Parallelization strategies:**  The documents describe how to parallelize the computation of attention matrices in both the forward and backward passes, using thread blocks to handle different row or column blocks of the attention matrix.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"AdaLUzg-93dM"},"execution_count":null,"outputs":[]}]}